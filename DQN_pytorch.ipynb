{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP80or84Z1rTsDlBGnS71R+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torchviz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vd7E8WSh6YwI","executionInfo":{"status":"ok","timestamp":1737658147699,"user_tz":300,"elapsed":10684,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}},"outputId":"4c58530f-7feb-4155-f0a3-69a3c17560aa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchviz\n","  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.5.1+cu121)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n","Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.3\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qBOhyI1rTCFK","executionInfo":{"status":"ok","timestamp":1737658169158,"user_tz":300,"elapsed":18995,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"outputs":[],"source":["# importing all necessary libraries\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","import graphviz\n","import torchviz\n","import seaborn as sns\n","import time"]},{"cell_type":"code","source":["class TicTacToe:\n","  def __init__(self):\n","    self.board = np.zeros((3, 3))\n","\n","  def reset(self):\n","    self.board = np.zeros((3, 3))\n","    return self.board\n","\n","  def check_win(self, player):\n","    # Check rows\n","    for i in range(3):\n","      if np.all(self.board[i, :] == player):\n","        return player\n","\n","    # Check columns\n","    for j in range(3):\n","      if np.all(self.board[:, j] == player):\n","        return player\n","\n","    #Check diagonal\n","    if np.all(np.diag(self.board) == player):\n","      return player\n","    if np.all(np.diag(np.fliplr(self.board)) == player):\n","      return player\n","\n","\n","    #Check tie\n","    if np.all(self.board != 0):\n","      return -1\n","\n","    return 0\n","\n","  # Check for empty places on board\n","  def valid_state(self, potential_action):\n","    valid_moves = []\n","    for i in range(len(self.board)):\n","        for j in range(len(self.board)):\n","\n","            if self.board[i][j] == 0:\n","                valid_moves.append((i, j))\n","    if potential_action in valid_moves:\n","        return True\n","    else:\n","        return False\n","\n","  # Check for empty places on board\n","  def possibilities(self):\n","    l = []\n","    print(self.board)\n","    for i in range(len(self.board)):\n","        for j in range(len(self.board)):\n","\n","            if self.board[i][j] == 0:\n","                l.append((i, j))\n","    return(l)\n","\n","  def random_step(self, player):\n","    selection = self.possibilities()\n","    current_loc = random.choice(selection)\n","    self.board[current_loc] = player\n","    done = self.check_win(player)\n","    reward = 0\n","    if done == 2:\n","      reward = -1\n","    return self.board, reward, done\n","\n","  def network_step(self, action, player):\n","    #print(action)\n","    row, col = action\n","    self.board[row, col] = player\n","    done = self.check_win(player)\n","    reward = 0\n","    if done == 1:\n","      reward = 1\n","    return self.board, reward, done"],"metadata":{"id":"xJTnNfR5TM5R","executionInfo":{"status":"ok","timestamp":1737658190766,"user_tz":300,"elapsed":129,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class PolicyNetwork(nn.Module):\n","    def __init__(self):\n","        super(PolicyNetwork, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(9, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 9),\n","            nn.Softmax(dim=-1),\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)"],"metadata":{"id":"dkSHDdmrTPTF","executionInfo":{"status":"ok","timestamp":1737658192558,"user_tz":300,"elapsed":113,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["tensor_to_tuple = {}\n","for i in range(9):\n","  tensor_to_tuple[i] = (i//3, i%3)\n","print(tensor_to_tuple)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bl1RFqCMoqg8","executionInfo":{"status":"ok","timestamp":1737658196049,"user_tz":300,"elapsed":113,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}},"outputId":"4c972241-1a08-4510-a101-2f1bace3557b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: (0, 0), 1: (0, 1), 2: (0, 2), 3: (1, 0), 4: (1, 1), 5: (1, 2), 6: (2, 0), 7: (2, 1), 8: (2, 2)}\n"]}]},{"cell_type":"code","source":["#Tic Tac Toe version\n","def compute_discounted_rewards(rewards, gamma=0.25):\n","    discounted_rewards = []\n","    discounted_reward = 0\n","    i = 0\n","    for reward in reversed(rewards):\n","        discounted_reward = reward + gamma * discounted_reward\n","        discounted_rewards.insert(0, discounted_reward)\n","        #print(i,\" discounted_rewards: \", discounted_rewards)\n","        i += 1\n","    discounted_rewards = torch.tensor(discounted_rewards)\n","\n","    discounted_rewards = discounted_rewards / (discounted_rewards.std() + 1e-6)\n","    # print(\"discounted rewards: \", discounted_rewards)\n","    # print(\"discounted rewards mean: \", discounted_rewards.mean())\n","    # print(\"discounted rewards std: \", discounted_rewards.std())\n","\n","    return discounted_rewards\n","\n","#%%debug\n","debug = True\n","\n","def train(env, policy, optimizer, episodes=100001):\n","\n","  i = 0\n","  games_won = 0\n","  games_lost = 0\n","  games_skipped = 0\n","  for episode in range(episodes):\n","    print(\"trajectory: \", i)\n","    state = env.reset()\n","    log_probs = []\n","    rewards = []\n","\n","\n","    done = 0\n","    not_done_counter = 0\n","    while done == 0:\n","        print(state)\n","        state = state.flatten()\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        if debug:\n","          print(\"inner episode loop \", not_done_counter, \" state: \", state)\n","\n","        probs = policy(state)\n","        if debug:\n","          print(\"inner episode loop \", not_done_counter, \" probs: \", probs)\n","\n","\n","        action_mask = torch.zeros_like(probs)\n","        for i in range(len(action_mask[0])): # Iterate through the actions in the mask\n","          if env.valid_state(tensor_to_tuple[i]):\n","            action_mask[0][i] = 1 # Apply mask to the specific action\n","        action_probabilities = Categorical(probs * action_mask)\n","        if debug:\n","          print(\"inner episode loop \", not_done_counter, \" action_probabilities: \", action_probabilities)\n","\n","        action = action_probabilities.sample()\n","\n","        if debug:\n","          print(\"inner episode loop \", not_done_counter, \" action: \", action)\n","          print(\"\")\n","          print(\"\")\n","          print(\"\")\n","\n","        #Sample again if action is invalid\n","        invalid_actions = 0\n","        while not env.valid_state(tensor_to_tuple[action.item()]):\n","          invalid_actions += 1\n","          if invalid_actions > 1000:\n","            break\n","          action = action_probabilities.sample()\n","          #print('action item: ', tensor_to_tuple[action.item()])\n","        #If 1000 actions sampled, and none of valid, skip to next trajectory\n","        if invalid_actions > 1000:\n","          games_skipped += 1\n","          print(\"Games skipped: \", games_skipped)\n","          break\n","\n","        if debug:\n","          print(\"inner episode loop \", not_done_counter, \"step input action.item(): \", action.item())\n","\n","        state, reward, done = env.network_step(tensor_to_tuple[action.item()], player = 1)\n","\n","        #Only generate random move is state is not terminal\n","        if done == 0:\n","          state, reward, done = env.random_step(player = 2)\n","        if debug:\n","          print(\"inner episode loop after step \", not_done_counter)\n","          print(\"step output state: \\n\", state)\n","          print(\"step output reward: \", reward)\n","          print(\"step output done: \", done)\n","          print(\"\")\n","          print(\"\")\n","          print(\"\")\n","\n","\n","\n","        log_probs.append(action_probabilities.log_prob(action))\n","        rewards.append(reward)\n","\n","        # Inside the train function, after an episode ends:\n","        if done != 0:\n","          i += 1\n","          print(state)\n","          print(\"DONE!!! \", done)\n","          if sum(rewards) == 1:\n","            games_won += 1\n","          elif sum(rewards) == -1:\n","            games_lost += 1\n","          episode_rewards.append(sum(rewards))\n","          discounted_rewards = compute_discounted_rewards(rewards)\n","\n","          policy_loss = []\n","          for log_prob, Gt in zip(log_probs, discounted_rewards):\n","              policy_loss.append(-log_prob * Gt)\n","          optimizer.zero_grad()\n","          policy_loss = torch.cat(policy_loss).sum()\n","          policy_loss.backward()\n","          optimizer.step()\n","          episode_losses.append(policy_loss.item())\n","\n","\n","          if episode % 50 == 0:\n","              print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n","              print(\"games_won: \", games_won)\n","              print(\"games lost: \", games_lost)\n","              print(\"games_skipped: \", games_skipped)\n","              print(\"\")\n","              print(\"\")\n","              print(\"\")\n","\n","          break\n","\n"],"metadata":{"id":"bgyT9sEDTSCA","executionInfo":{"status":"ok","timestamp":1737661258450,"user_tz":300,"elapsed":115,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#Globals\n","episode_rewards = []\n","episode_losses = []\n","\n","#Driver code for training the model\n","my_env = TicTacToe()\n","print(\"\")\n","policy = PolicyNetwork()\n","print(\"Policy: \", policy)\n","total_params = sum(p.numel() for p in policy.parameters())\n","print(f\"Number of parameters: {total_params}\")\n","optimizer = optim.Adam(policy.parameters(), lr=1e-6)\n","\n","#train(env, policy, optimizer)\n","train(my_env, policy, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJryd-8DTU1a","outputId":"d7194c2c-48c6-4b0d-d0a5-61fd11f8e615","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","\n","[[2. 1. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 1., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1299, 0.1234, 0.1005, 0.0813, 0.1393, 0.0868, 0.1449, 0.0877, 0.1062]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[2. 1. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [1. 2. 0.]\n"," [2. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 0.]\n"," [1. 2. 0.]\n"," [2. 0. 1.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 1., 2., 0., 2., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1178, 0.1094, 0.0691, 0.0702, 0.1611, 0.0818, 0.2106, 0.0705, 0.1094]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[2. 1. 0.]\n"," [1. 2. 0.]\n"," [2. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 2.]\n"," [1. 2. 0.]\n"," [2. 1. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 1. 2.]\n"," [1. 2. 0.]\n"," [2. 1. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1189]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1092, 0.1190, 0.1022, 0.0862, 0.1523, 0.0954, 0.1283, 0.0907, 0.1168]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[2. 1. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 0., 0., 0., 1., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1066, 0.1216, 0.0940, 0.0932, 0.1291, 0.0908, 0.1756, 0.0856, 0.1035]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[2. 1. 0.]\n"," [0. 0. 0.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [0. 0. 2.]\n"," [1. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 0.]\n"," [0. 0. 2.]\n"," [1. 1. 2.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 0., 0., 2., 1., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1190, 0.0986, 0.0714, 0.0903, 0.1513, 0.0931, 0.1651, 0.0980, 0.1134]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[2. 1. 0.]\n"," [1. 0. 2.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 2.]\n"," [1. 0. 2.]\n"," [1. 1. 2.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 1. 2.]\n"," [1. 0. 2.]\n"," [1. 1. 2.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1189]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 0., 0., 0., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.0976, 0.1130, 0.0883, 0.0910, 0.1554, 0.1115, 0.1402, 0.0873, 0.1159]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[2. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[2., 2., 1., 0., 0., 0., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1301, 0.1078, 0.0890, 0.0944, 0.1120, 0.1041, 0.1632, 0.0950, 0.1044]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[2. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 2. 1.]]\n","inner episode loop  0  state:  tensor([[2., 2., 1., 0., 0., 0., 1., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1395, 0.0845, 0.0731, 0.0849, 0.1214, 0.0897, 0.2069, 0.0961, 0.1039]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[2. 2. 1.]\n"," [1. 0. 0.]\n"," [1. 2. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 1.]]\n","inner episode loop  0  state:  tensor([[2., 2., 1., 1., 0., 2., 1., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1464, 0.0890, 0.0642, 0.0849, 0.1508, 0.0869, 0.1783, 0.1017, 0.0978]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [1. 1. 2.]\n"," [1. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [1. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1189]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1178, 0.0903, 0.0963, 0.1116, 0.1286, 0.0972, 0.1495, 0.1092, 0.0995]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[1. 0. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 1., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1117, 0.1046, 0.1072, 0.1094, 0.1361, 0.0767, 0.1825, 0.0995, 0.0723]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[1. 1. 2.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [1. 0. 2.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 1. 2.]\n"," [1. 0. 2.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 1., 2., 1., 0., 2., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1338, 0.1051, 0.0972, 0.0961, 0.1504, 0.0773, 0.1735, 0.0935, 0.0730]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[1. 1. 2.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [1. 2. 2.]\n"," [2. 0. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 1. 2.]\n"," [1. 2. 2.]\n"," [2. 0. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1189]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 0., 1., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1052, 0.0913, 0.1051, 0.0792, 0.1469, 0.1163, 0.1648, 0.0899, 0.1013]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 0., 2., 0., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1087, 0.1000, 0.1006, 0.0822, 0.1671, 0.0920, 0.1757, 0.0803, 0.0935]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[1. 0. 0.]\n"," [2. 1. 1.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [2. 1. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [2. 1. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 2., 1., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1508, 0.1054, 0.0704, 0.0626, 0.1631, 0.0828, 0.1981, 0.0714, 0.0955]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 1., 2., 1., 1., 2., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1354, 0.1022, 0.0549, 0.0632, 0.1568, 0.0792, 0.2381, 0.0749, 0.0954]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1189]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 1., 0., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1257, 0.0881, 0.0605, 0.0980, 0.1607, 0.1145, 0.1244, 0.0946, 0.1334]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 0., 0., 1., 0., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1555, 0.0937, 0.0595, 0.0806, 0.1431, 0.1074, 0.1533, 0.0850, 0.1217]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[1. 2. 0.]\n"," [0. 0. 1.]\n"," [1. 2. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [0. 2. 1.]\n"," [1. 2. 0.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 2. 0.]\n"," [0. 2. 1.]\n"," [1. 2. 0.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 0., 0., 1., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1011, 0.1075, 0.0880, 0.0857, 0.1754, 0.1108, 0.1139, 0.0937, 0.1238]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[2. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 2., 0., 0., 1., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1213, 0.0780, 0.0859, 0.1042, 0.1558, 0.0945, 0.1472, 0.1043, 0.1086]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[2. 1. 2.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 2.]\n"," [0. 0. 1.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 2.]\n"," [0. 0. 1.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 1., 2., 0., 0., 1., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1453, 0.1015, 0.0981, 0.0955, 0.1434, 0.0725, 0.1631, 0.0957, 0.0849]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 1. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 0., 0., 0., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1374, 0.1159, 0.0857, 0.0894, 0.1136, 0.0969, 0.1690, 0.0890, 0.1031]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 2.]\n"," [0. 0. 0.]\n"," [1. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 2.]\n"," [0. 0. 0.]\n"," [1. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 2., 2., 0., 0., 0., 1., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1178, 0.0967, 0.0721, 0.1083, 0.1001, 0.0898, 0.2234, 0.1023, 0.0895]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 2. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 2. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1293, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 1., 0., 0., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1255, 0.1223, 0.0938, 0.1015, 0.1346, 0.0885, 0.1497, 0.0890, 0.0951]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 1. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 0.]\n"," [1. 0. 2.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 0.]\n"," [1. 0. 2.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 1., 0., 1., 0., 2., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1443, 0.1140, 0.0871, 0.0873, 0.1613, 0.0843, 0.1644, 0.0762, 0.0811]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 1. 0.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 2.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 2.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 1., 2., 1., 0., 2., 2., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1236, 0.0892, 0.0823, 0.0983, 0.1462, 0.0778, 0.2199, 0.0931, 0.0696]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 1. 2.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [1. 2. 2.]\n"," [2. 0. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 1. 2.]\n"," [1. 2. 2.]\n"," [2. 0. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 0., 0., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1206, 0.1245, 0.1047, 0.0940, 0.1417, 0.0897, 0.1366, 0.0861, 0.1020]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[1. 0. 1.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 1.]\n"," [2. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 1.]\n"," [2. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 1., 2., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1055, 0.1007, 0.1028, 0.0922, 0.1418, 0.0854, 0.1951, 0.0936, 0.0829]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[1. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 0. 2.]]\n","inner episode loop  0  state:  tensor([[1., 0., 1., 2., 1., 0., 2., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0890, 0.0790, 0.0772, 0.0843, 0.1325, 0.0819, 0.2815, 0.0984, 0.0762]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[1. 0. 1.]\n"," [2. 1. 1.]\n"," [2. 0. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 0. 2.]]\n","inner episode loop  0  state:  tensor([[1., 2., 1., 2., 1., 1., 2., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1252, 0.0846, 0.0627, 0.0793, 0.1402, 0.0769, 0.2738, 0.0789, 0.0783]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 1. 2.]]\n","step output reward:  0\n","step output done:  -1\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 1. 2.]]\n","DONE!!!  -1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 1., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1020, 0.0991, 0.0918, 0.0779, 0.1623, 0.1058, 0.1515, 0.0925, 0.1171]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 1., 0., 0., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1207, 0.1084, 0.0918, 0.0842, 0.1462, 0.0826, 0.1705, 0.0890, 0.1065]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[2. 0. 0.]\n"," [1. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 2.]\n"," [1. 1. 0.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 2.]\n"," [1. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 2., 1., 1., 0., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1146, 0.0987, 0.0803, 0.0893, 0.1540, 0.0647, 0.2152, 0.0988, 0.0843]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 2.]\n"," [1. 1. 1.]\n"," [2. 1. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 0. 2.]\n"," [1. 1. 1.]\n"," [2. 1. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1178, 0.0903, 0.0964, 0.1116, 0.1286, 0.0972, 0.1494, 0.1092, 0.0994]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 2., 0., 0., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1396, 0.0906, 0.0722, 0.1038, 0.1122, 0.1002, 0.1732, 0.1014, 0.1069]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[1. 2. 2.]\n"," [0. 0. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [0. 2. 0.]\n"," [1. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 2.]\n"," [0. 2. 0.]\n"," [1. 1. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 2., 0., 2., 0., 1., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1550, 0.0918, 0.0481, 0.0854, 0.1354, 0.0740, 0.2218, 0.0774, 0.1111]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [1. 2. 0.]\n"," [1. 1. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 2. 2.]\n"," [1. 2. 0.]\n"," [1. 1. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1308, 0.1147, 0.0966, 0.1060, 0.1489, 0.0881, 0.1393, 0.0811, 0.0944]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 0.]\n"," [0. 2. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 0.]\n"," [0. 2. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 1., 0., 0., 2., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1419, 0.1018, 0.0667, 0.0782, 0.1771, 0.0862, 0.1791, 0.0616, 0.1074]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 1. 1.]\n"," [0. 2. 1.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 1.]\n"," [0. 2. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 1.]\n"," [0. 2. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 1., 1., 0., 2., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1399, 0.1121, 0.0731, 0.0812, 0.1602, 0.0796, 0.1596, 0.0806, 0.1137]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 1.]\n"," [0. 2. 1.]\n"," [2. 0. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 1. 1.]\n"," [0. 2. 1.]\n"," [2. 0. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 0., 1., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1052, 0.0913, 0.1052, 0.0792, 0.1469, 0.1163, 0.1648, 0.0899, 0.1012]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 2., 0., 1., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1576, 0.1003, 0.0870, 0.0721, 0.1473, 0.1031, 0.1570, 0.0802, 0.0955]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 1.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 2., 0., 1., 2., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1487, 0.0961, 0.0771, 0.0757, 0.1519, 0.0822, 0.2077, 0.0806, 0.0801]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [2. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [2. 2. 1.]\n"," [2. 1. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [2. 2. 1.]\n"," [2. 1. 1.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 2., 2., 1., 2., 1., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1268, 0.0907, 0.0521, 0.0538, 0.1730, 0.0812, 0.2617, 0.0604, 0.1004]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 2. 1.]\n"," [2. 1. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 2. 1.]\n"," [2. 1. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 1. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 1., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1047, 0.0945, 0.0965, 0.0670, 0.1470, 0.1203, 0.1753, 0.0838, 0.1109]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [2. 1. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 1., 2., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.0992, 0.0857, 0.0948, 0.0672, 0.1916, 0.1111, 0.1728, 0.0758, 0.1020]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[0. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 2., 1., 2., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1282, 0.0943, 0.0812, 0.0629, 0.1951, 0.1000, 0.1521, 0.0746, 0.1116]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 2., 1., 2., 1., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1184, 0.0816, 0.0556, 0.0638, 0.1926, 0.1034, 0.1900, 0.0809, 0.1136]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 1.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 1. 1.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 1., 0., 0., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1180, 0.0898, 0.0521, 0.0809, 0.1709, 0.1083, 0.1471, 0.0880, 0.1449]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 1., 2., 0., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1199, 0.0719, 0.0488, 0.0722, 0.1848, 0.1183, 0.1456, 0.0933, 0.1453]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [1. 2. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 1. 2.]\n"," [1. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 1. 2.]\n"," [1. 2. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 1., 2., 1., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1180, 0.0618, 0.0588, 0.0922, 0.1874, 0.0810, 0.1888, 0.0958, 0.1162]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 1. 2.]\n"," [1. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 1. 2.]\n"," [1. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 0., 0., 0., 2., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1156, 0.1047, 0.0890, 0.0901, 0.1695, 0.1076, 0.1080, 0.0967, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[1. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 0., 0., 1., 2., 0., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1217, 0.0743, 0.0519, 0.0786, 0.2127, 0.1158, 0.1206, 0.0842, 0.1404]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1163, 0.1017, 0.0916, 0.1059, 0.1621, 0.0901, 0.1473, 0.0832, 0.1019]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 0., 1., 0., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1414, 0.1045, 0.0676, 0.0816, 0.1357, 0.0928, 0.1950, 0.0714, 0.1100]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 2. 0.]\n"," [0. 1. 0.]\n"," [2. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [0. 1. 2.]\n"," [2. 1. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [0. 1. 2.]\n"," [2. 1. 1.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 0., 1., 2., 2., 1., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1614, 0.0947, 0.0553, 0.0733, 0.1554, 0.0868, 0.2027, 0.0715, 0.0989]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","inner episode loop  0  state:  tensor([[2., 2., 0., 1., 1., 2., 2., 1., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1583, 0.0913, 0.0614, 0.0727, 0.1728, 0.0861, 0.1774, 0.0754, 0.1047]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","step output reward:  0\n","step output done:  -1\n","\n","\n","\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","DONE!!!  -1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 1., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1124, 0.1114, 0.0869, 0.0947, 0.1706, 0.0890, 0.1573, 0.0781, 0.0995]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 1., 0., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1150, 0.0878, 0.0773, 0.1092, 0.1680, 0.0676, 0.2039, 0.0796, 0.0916]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 2.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 0., 1., 0., 2., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0998, 0.0716, 0.0725, 0.1017, 0.1347, 0.0725, 0.2613, 0.1044, 0.0816]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[1. 0. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 2.]]\n","inner episode loop  0  state:  tensor([[1., 2., 2., 0., 1., 1., 2., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1212, 0.0797, 0.0576, 0.0919, 0.1197, 0.0752, 0.2746, 0.0935, 0.0868]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [1. 1. 1.]\n"," [2. 1. 2.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 2. 2.]\n"," [1. 1. 1.]\n"," [2. 1. 2.]]\n","DONE!!!  1\n","Episode 300, Total Reward: 1\n","games_won:  174\n","games lost:  82\n","games_skipped:  0\n","\n","\n","\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 0., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1212, 0.0803, 0.0863, 0.1050, 0.1320, 0.0912, 0.1635, 0.1033, 0.1172]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 2., 0., 1., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1187, 0.0854, 0.0691, 0.1020, 0.1524, 0.0861, 0.1759, 0.1054, 0.1049]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[2. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 2.]\n"," [0. 1. 0.]\n"," [2. 1. 1.]]\n","inner episode loop  0  state:  tensor([[2., 0., 2., 0., 1., 0., 2., 1., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1179, 0.0894, 0.0808, 0.1001, 0.1467, 0.0689, 0.2139, 0.1003, 0.0820]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[2. 0. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 2. 2.]\n"," [0. 1. 1.]\n"," [2. 1. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 1., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1098, 0.1133, 0.1004, 0.0968, 0.1477, 0.0897, 0.1663, 0.0854, 0.0908]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [1. 0. 0.]\n"," [2. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [2. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 1., 1., 0., 0., 2., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1085, 0.0961, 0.0760, 0.0998, 0.1654, 0.0818, 0.1982, 0.0824, 0.0918]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 1.]\n"," [1. 1. 0.]\n"," [2. 2. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [1. 1. 0.]\n"," [2. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [1. 1. 0.]\n"," [2. 2. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 1., 1., 0., 2., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1203, 0.0957, 0.0685, 0.0770, 0.1647, 0.0704, 0.2195, 0.0870, 0.0968]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[2. 1. 1.]\n"," [1. 1. 0.]\n"," [2. 2. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 1.]\n"," [1. 1. 0.]\n"," [2. 2. 2.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 1. 1.]\n"," [1. 1. 0.]\n"," [2. 2. 2.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0955, 0.0952, 0.0714, 0.0848, 0.1148, 0.1045, 0.1944, 0.1202, 0.1191]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 1., 0., 0., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1015, 0.0697, 0.0663, 0.0848, 0.1172, 0.0855, 0.2446, 0.1204, 0.1100]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [2. 1. 0.]\n"," [1. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [2. 1. 0.]\n"," [1. 1. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 2., 1., 0., 1., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0812, 0.0700, 0.0680, 0.0714, 0.1166, 0.0856, 0.2974, 0.1277, 0.0819]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 2.]\n"," [2. 1. 0.]\n"," [1. 1. 2.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[0. 1. 2.]\n"," [2. 1. 0.]\n"," [1. 1. 2.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1178, 0.0903, 0.0964, 0.1116, 0.1287, 0.0972, 0.1494, 0.1092, 0.0994]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 2., 0., 1., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1006, 0.0820, 0.1067, 0.0886, 0.1398, 0.1081, 0.1692, 0.1120, 0.0930]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[1. 0. 2.]\n"," [2. 0. 1.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [2. 0. 1.]\n"," [1. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [2. 0. 1.]\n"," [1. 0. 2.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 2., 0., 1., 1., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0906, 0.0731, 0.0935, 0.0970, 0.1236, 0.0926, 0.2284, 0.1299, 0.0713]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[1. 0. 2.]\n"," [2. 0. 1.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [2. 0. 1.]\n"," [1. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 2.]\n"," [2. 0. 1.]\n"," [1. 1. 2.]]\n","inner episode loop  0  state:  tensor([[1., 2., 2., 2., 0., 1., 1., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1123, 0.0819, 0.0636, 0.0938, 0.1054, 0.0857, 0.2796, 0.1077, 0.0701]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 2.]\n"," [2. 1. 1.]\n"," [1. 1. 2.]]\n","step output reward:  0\n","step output done:  -1\n","\n","\n","\n","[[1. 2. 2.]\n"," [2. 1. 1.]\n"," [1. 1. 2.]]\n","DONE!!!  -1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 1., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1492, 0.1056, 0.0898, 0.0830, 0.1160, 0.1035, 0.1715, 0.0826, 0.0988]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [1. 0. 2.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [1. 0. 2.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 1., 0., 2., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1690, 0.1205, 0.0750, 0.0758, 0.1447, 0.0885, 0.1591, 0.0846, 0.0827]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 2. 0.]\n"," [1. 1. 2.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 2., 1., 1., 2., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1506, 0.1077, 0.0683, 0.0798, 0.1409, 0.0883, 0.1930, 0.0859, 0.0855]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 0., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1077, 0.0993, 0.0869, 0.0789, 0.1314, 0.1176, 0.1759, 0.0910, 0.1112]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [2. 0. 0.]\n"," [1. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [2. 0. 0.]\n"," [1. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 2., 0., 0., 1., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1343, 0.1022, 0.0774, 0.0729, 0.1251, 0.0969, 0.2216, 0.0773, 0.0922]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 2. 0.]\n"," [2. 1. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 2., 1., 2., 1., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1486, 0.1082, 0.0587, 0.0575, 0.1754, 0.0967, 0.2007, 0.0650, 0.0891]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[0. 2. 1.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1343, 0.1119, 0.0798, 0.0984, 0.1086, 0.1049, 0.1642, 0.0952, 0.1027]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 2., 0., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1339, 0.0997, 0.0751, 0.0838, 0.1184, 0.1095, 0.2041, 0.0877, 0.0878]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 2. 1.]\n"," [2. 0. 0.]\n"," [0. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [2. 2. 0.]\n"," [0. 1. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [2. 2. 0.]\n"," [0. 1. 1.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 2., 2., 0., 0., 1., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1289, 0.0923, 0.0523, 0.0567, 0.1287, 0.0937, 0.2752, 0.0701, 0.1021]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 2. 1.]\n"," [2. 2. 0.]\n"," [0. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 2. 2.]\n"," [0. 1. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 2. 2.]\n"," [0. 1. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1023, 0.0931, 0.0864, 0.0861, 0.1107, 0.1025, 0.2080, 0.1132, 0.0977]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 1., 0., 0., 0., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0953, 0.0994, 0.0870, 0.0934, 0.1256, 0.1073, 0.1972, 0.1028, 0.0920]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[2. 0. 1.]\n"," [1. 1. 0.]\n"," [0. 0. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [1. 1. 2.]\n"," [0. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [1. 1. 2.]\n"," [0. 0. 2.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 1., 1., 2., 0., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0972, 0.0759, 0.0756, 0.0811, 0.1602, 0.1155, 0.1775, 0.1033, 0.1139]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [1. 1. 2.]\n"," [1. 0. 2.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 0. 1.]\n"," [1. 1. 2.]\n"," [1. 0. 2.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 1., 2., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1031, 0.1021, 0.0743, 0.0667, 0.1682, 0.1108, 0.1748, 0.0777, 0.1222]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[0. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 2.]]\n","inner episode loop  0  state:  tensor([[0., 1., 0., 1., 2., 0., 0., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1095, 0.0881, 0.0663, 0.0590, 0.1267, 0.1034, 0.2636, 0.0673, 0.1161]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 2. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 2. 2.]]\n","inner episode loop  0  state:  tensor([[1., 1., 0., 1., 2., 0., 0., 2., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1059, 0.0698, 0.0469, 0.0478, 0.1413, 0.1016, 0.2870, 0.0712, 0.1285]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 0.]\n"," [1. 2. 0.]\n"," [1. 2. 2.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 1. 0.]\n"," [1. 2. 0.]\n"," [1. 2. 2.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 1., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1492, 0.1056, 0.0898, 0.0830, 0.1160, 0.1035, 0.1715, 0.0827, 0.0988]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 1., 2., 0., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1413, 0.1161, 0.0631, 0.0661, 0.1448, 0.0884, 0.2062, 0.0636, 0.1104]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 2. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1148, 0.0906, 0.0797, 0.0896, 0.1667, 0.1027, 0.1411, 0.0933, 0.1216]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 2., 0., 0., 1., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1137, 0.0978, 0.0813, 0.0728, 0.1506, 0.0862, 0.1933, 0.0921, 0.1122]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[2. 1. 0.]\n"," [2. 0. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [2. 0. 2.]\n"," [1. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 0.]\n"," [2. 0. 2.]\n"," [1. 1. 0.]]\n","inner episode loop  0  state:  tensor([[2., 1., 0., 2., 0., 2., 1., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1283, 0.1003, 0.0839, 0.0717, 0.1879, 0.0964, 0.1387, 0.0882, 0.1047]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 1. 0.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 1., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1492, 0.1056, 0.0898, 0.0830, 0.1160, 0.1035, 0.1716, 0.0827, 0.0988]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 0.]\n"," [1. 0. 2.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 0.]\n"," [1. 0. 2.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 0., 1., 0., 2., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1638, 0.1111, 0.0641, 0.0742, 0.1489, 0.1015, 0.1656, 0.0819, 0.0888]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 2. 0.]\n"," [1. 0. 2.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [1. 2. 2.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [1. 2. 2.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 1., 2., 2., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1595, 0.0959, 0.0488, 0.0553, 0.1800, 0.1030, 0.1617, 0.0611, 0.1347]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 2. 0.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 2., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1163, 0.1017, 0.0916, 0.1059, 0.1621, 0.0901, 0.1473, 0.0832, 0.1018]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 1. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 1., 2., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1016, 0.0791, 0.0704, 0.1038, 0.1607, 0.0834, 0.2011, 0.0997, 0.1002]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 1. 2.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 0., 0., 1., 2., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1093, 0.0853, 0.0839, 0.1026, 0.1474, 0.0832, 0.1963, 0.1029, 0.0891]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[2. 0. 1.]\n"," [0. 1. 1.]\n"," [2. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [2. 1. 1.]\n"," [2. 1. 2.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 0. 1.]\n"," [2. 1. 1.]\n"," [2. 1. 2.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 1., 0., 0., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1180, 0.0898, 0.0521, 0.0809, 0.1709, 0.1083, 0.1471, 0.0880, 0.1449]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [2. 1. 0.]\n"," [0. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 1.]\n"," [2. 1. 0.]\n"," [0. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 1., 2., 1., 0., 0., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.0978, 0.0943, 0.0622, 0.0700, 0.1557, 0.0996, 0.2103, 0.0952, 0.1148]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 1.]\n"," [2. 1. 0.]\n"," [0. 2. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 2. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 1., 2., 1., 0., 2., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.0835, 0.0818, 0.0594, 0.0701, 0.1648, 0.0800, 0.2833, 0.0917, 0.0855]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[0. 0. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 2., 0., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1025, 0.0960, 0.0686, 0.0733, 0.1515, 0.1070, 0.1912, 0.0749, 0.1349]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 2., 2., 1., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1106, 0.0779, 0.0681, 0.0795, 0.1832, 0.0970, 0.1858, 0.0694, 0.1284]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 0., 2., 2., 1., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1065, 0.0816, 0.0724, 0.0847, 0.1833, 0.0980, 0.1635, 0.0831, 0.1270]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[2. 0. 1.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","inner episode loop  0  state:  tensor([[2., 2., 1., 1., 2., 2., 1., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1411, 0.0941, 0.0602, 0.0701, 0.1717, 0.0893, 0.1846, 0.0740, 0.1150]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 2., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1114, 0.0982, 0.0576, 0.0726, 0.1747, 0.1067, 0.1621, 0.0762, 0.1404]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 0., 2., 0., 0., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1530, 0.1053, 0.0524, 0.0649, 0.1413, 0.1005, 0.1722, 0.0676, 0.1428]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[1. 2. 0.]\n"," [0. 2. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 0.]\n"," [2. 2. 0.]\n"," [1. 1. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 2. 0.]\n"," [2. 2. 0.]\n"," [1. 1. 0.]]\n","inner episode loop  0  state:  tensor([[1., 2., 0., 2., 2., 0., 1., 1., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1475, 0.1038, 0.0580, 0.0527, 0.1516, 0.0863, 0.2173, 0.0635, 0.1194]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[1. 2. 1.]\n"," [2. 2. 0.]\n"," [1. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 2. 1.]\n"," [2. 2. 2.]\n"," [1. 1. 0.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 2. 1.]\n"," [2. 2. 2.]\n"," [1. 1. 0.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 0., 0., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1118, 0.0791, 0.0946, 0.1013, 0.1105, 0.0901, 0.1791, 0.1309, 0.1025]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 0., 0., 2., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1155, 0.0777, 0.0874, 0.0921, 0.1514, 0.1013, 0.1492, 0.1185, 0.1068]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[1. 0. 2.]\n"," [0. 0. 2.]\n"," [1. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 2.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","inner episode loop  0  state:  tensor([[1., 0., 2., 0., 2., 2., 1., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1104, 0.0696, 0.0712, 0.0881, 0.1800, 0.0845, 0.1941, 0.0874, 0.1146]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 2.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 0. 2.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 1., 0., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1139, 0.1095, 0.1043, 0.1173, 0.1420, 0.0830, 0.1590, 0.0866, 0.0844]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 1.]\n"," [0. 1. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [0. 1. 2.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 1.]\n"," [0. 1. 2.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 1., 0., 1., 2., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1332, 0.0957, 0.0899, 0.0954, 0.1780, 0.0802, 0.1648, 0.0695, 0.0935]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 1.]\n"," [0. 1. 2.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 1.]\n"," [0. 1. 2.]\n"," [2. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 1.]\n"," [0. 1. 2.]\n"," [2. 2. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 1., 0., 1., 2., 2., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1333, 0.0783, 0.0686, 0.0858, 0.2138, 0.0808, 0.1558, 0.0702, 0.1135]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 1.]\n"," [0. 1. 2.]\n"," [2. 2. 0.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 1. 1.]\n"," [0. 1. 2.]\n"," [2. 2. 0.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 0., 0., 0., 1., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1011, 0.1075, 0.0880, 0.0857, 0.1755, 0.1108, 0.1139, 0.0937, 0.1238]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[2. 0. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 2., 0., 0., 0., 1., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1527, 0.1266, 0.0844, 0.0855, 0.1379, 0.0950, 0.1169, 0.0870, 0.1140]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[2. 2. 0.]\n"," [1. 0. 1.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 0.]\n"," [1. 0. 1.]\n"," [1. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 2. 0.]\n"," [1. 0. 1.]\n"," [1. 0. 2.]]\n","inner episode loop  0  state:  tensor([[2., 2., 0., 1., 0., 1., 1., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1335, 0.1085, 0.0836, 0.0887, 0.1406, 0.0869, 0.1854, 0.0807, 0.0921]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","inner episode loop after step  0\n","step output state: \n"," [[2. 2. 0.]\n"," [1. 1. 1.]\n"," [1. 0. 2.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 2. 0.]\n"," [1. 1. 1.]\n"," [1. 0. 2.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 1., 0., 2., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1151, 0.0971, 0.0955, 0.0842, 0.1676, 0.1101, 0.1387, 0.0902, 0.1015]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 1., 0., 2., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1677, 0.1073, 0.0739, 0.0841, 0.1345, 0.1034, 0.1553, 0.0916, 0.0822]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 1., 0., 2., 1., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1530, 0.0997, 0.0582, 0.0821, 0.1524, 0.0950, 0.1835, 0.0852, 0.0908]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[0. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 0., 0., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1072, 0.0963, 0.1066, 0.0816, 0.1343, 0.1078, 0.1803, 0.0889, 0.0969]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[0. 1. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 2.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 2.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 1., 2., 2., 0., 0., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1058, 0.0957, 0.0966, 0.0965, 0.1171, 0.0991, 0.2030, 0.1073, 0.0790]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 1. 2.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [2. 0. 2.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 1. 2.]\n"," [2. 0. 2.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 1., 2., 2., 0., 2., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1237, 0.0928, 0.1047, 0.0875, 0.1512, 0.0944, 0.1647, 0.1040, 0.0772]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[1. 1. 2.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [2. 1. 2.]\n"," [1. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 1. 2.]\n"," [2. 1. 2.]\n"," [1. 2. 0.]]\n","inner episode loop  0  state:  tensor([[1., 1., 2., 2., 1., 2., 1., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1120, 0.0921, 0.0628, 0.0774, 0.1833, 0.0824, 0.2062, 0.0871, 0.0967]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 1. 2.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1343, 0.1119, 0.0797, 0.0984, 0.1086, 0.1049, 0.1642, 0.0952, 0.1027]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 2. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 1., 0., 2., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1677, 0.1073, 0.0739, 0.0841, 0.1345, 0.1034, 0.1553, 0.0916, 0.0822]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 2. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [0. 2. 1.]]\n","inner episode loop  0  state:  tensor([[0., 2., 1., 1., 0., 2., 0., 2., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1377, 0.0895, 0.0512, 0.0807, 0.1411, 0.1044, 0.2100, 0.0934, 0.0921]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 2. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[0. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 2., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1185, 0.0962, 0.0833, 0.0928, 0.1518, 0.0991, 0.1369, 0.1087, 0.1128]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [2. 1. 2.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [2. 1. 2.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 2., 1., 2., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.0965, 0.0773, 0.0855, 0.0676, 0.1724, 0.1226, 0.1894, 0.0829, 0.1059]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [2. 1. 2.]\n"," [0. 0. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 0. 0.]\n"," [2. 1. 2.]\n"," [0. 0. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","inner episode loop  0  state:  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1008, 0.1099, 0.0800, 0.0905, 0.1057, 0.0968, 0.2009, 0.1024, 0.1130]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 1. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 1. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 2.]]\n","inner episode loop  0  state:  tensor([[0., 1., 0., 0., 2., 0., 1., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1023, 0.0882, 0.0618, 0.0691, 0.1353, 0.0897, 0.2552, 0.0744, 0.1241]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 1. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [0. 2. 0.]\n"," [1. 0. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 1. 2.]\n"," [0. 2. 0.]\n"," [1. 0. 2.]]\n","inner episode loop  0  state:  tensor([[1., 1., 2., 0., 2., 0., 1., 0., 2.]])\n","inner episode loop  0  probs:  tensor([[0.1092, 0.0835, 0.0612, 0.0976, 0.1124, 0.0796, 0.2654, 0.0925, 0.0986]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[1. 1. 2.]\n"," [0. 2. 0.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [0. 2. 2.]\n"," [1. 1. 2.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 1. 2.]\n"," [0. 2. 2.]\n"," [1. 1. 2.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1127, 0.1044, 0.0940, 0.1033, 0.1430, 0.1047, 0.1340, 0.1000, 0.1040]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[2. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 1.]\n"," [0. 1. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 1.]\n"," [0. 1. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 0., 1., 0., 1., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1152, 0.1238, 0.0932, 0.0945, 0.1471, 0.0802, 0.1588, 0.0892, 0.0979]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[2. 1. 1.]\n"," [0. 1. 0.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 1.]\n"," [0. 1. 2.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 1. 1.]\n"," [0. 1. 2.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[2., 1., 1., 0., 1., 2., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1375, 0.1140, 0.0862, 0.0843, 0.1648, 0.0829, 0.1429, 0.0785, 0.1089]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[2. 1. 1.]\n"," [0. 1. 2.]\n"," [2. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 1.]\n"," [2. 1. 2.]\n"," [2. 0. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[2. 1. 1.]\n"," [2. 1. 2.]\n"," [2. 0. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 2., 0., 1., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1081, 0.1103, 0.0721, 0.0793, 0.1737, 0.0985, 0.1680, 0.0751, 0.1150]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[0. 0. 0.]\n"," [0. 2. 1.]\n"," [1. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 2. 1.]\n"," [1. 2. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 2. 1.]\n"," [1. 2. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 2., 1., 1., 2., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1124, 0.0794, 0.0485, 0.0683, 0.2249, 0.1011, 0.1564, 0.0720, 0.1369]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([3])\n","\n","\n","\n","inner episode loop  0 step input action.item():  3\n","[[0. 0. 0.]\n"," [1. 2. 1.]\n"," [1. 2. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [1. 2. 1.]\n"," [1. 2. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [1. 2. 1.]\n"," [1. 2. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 1., 2., 1., 1., 2., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0869, 0.0613, 0.0467, 0.0495, 0.1834, 0.0994, 0.2606, 0.0836, 0.1286]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [1. 2. 1.]\n"," [1. 2. 2.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[1. 0. 0.]\n"," [1. 2. 1.]\n"," [1. 2. 2.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([0])\n","\n","\n","\n","inner episode loop  0 step input action.item():  0\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 0., 0., 0., 0., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1206, 0.1245, 0.1047, 0.0940, 0.1418, 0.0898, 0.1366, 0.0861, 0.1019]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([5])\n","\n","\n","\n","inner episode loop  0 step input action.item():  5\n","[[1. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 0., 0., 2., 0., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1086, 0.1000, 0.1006, 0.0821, 0.1673, 0.0920, 0.1758, 0.0802, 0.0933]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","[[1. 1. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[1. 1. 2.]\n"," [2. 0. 1.]\n"," [2. 0. 0.]]\n","inner episode loop  0  state:  tensor([[1., 1., 2., 2., 0., 1., 2., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1209, 0.0978, 0.0948, 0.0942, 0.1424, 0.0818, 0.1976, 0.0994, 0.0711]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[1. 1. 2.]\n"," [2. 0. 1.]\n"," [2. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[1. 1. 2.]\n"," [2. 2. 1.]\n"," [2. 0. 1.]]\n","step output reward:  -1\n","step output done:  2\n","\n","\n","\n","[[1. 1. 2.]\n"," [2. 2. 1.]\n"," [2. 0. 1.]]\n","DONE!!!  2\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([4])\n","\n","\n","\n","inner episode loop  0 step input action.item():  4\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 1., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1170, 0.0839, 0.0777, 0.1011, 0.1415, 0.0869, 0.1757, 0.1030, 0.1133]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([8])\n","\n","\n","\n","inner episode loop  0 step input action.item():  8\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 0. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 0. 1.]]\n","inner episode loop  0  state:  tensor([[0., 0., 2., 0., 1., 2., 0., 0., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1208, 0.0671, 0.0767, 0.0885, 0.1547, 0.0945, 0.1689, 0.1094, 0.1194]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 1.]]\n","inner episode loop after step  0\n","step output state: \n"," [[2. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 1.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[2. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 1.]]\n","inner episode loop  0  state:  tensor([[2., 0., 2., 0., 1., 2., 0., 1., 1.]])\n","inner episode loop  0  probs:  tensor([[0.1063, 0.0674, 0.0661, 0.0945, 0.1791, 0.0981, 0.1608, 0.1090, 0.1187]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([1])\n","\n","\n","\n","inner episode loop  0 step input action.item():  1\n","inner episode loop after step  0\n","step output state: \n"," [[2. 1. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 1.]]\n","step output reward:  1\n","step output done:  1\n","\n","\n","\n","[[2. 1. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 1.]]\n","DONE!!!  1\n","trajectory:  9\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","inner episode loop  0  probs:  tensor([[0.1111, 0.1059, 0.0937, 0.1039, 0.1294, 0.1094, 0.1264, 0.1015, 0.1188]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([7])\n","\n","\n","\n","inner episode loop  0 step input action.item():  7\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0955, 0.0952, 0.0714, 0.0848, 0.1149, 0.1045, 0.1945, 0.1202, 0.1190]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([6])\n","\n","\n","\n","inner episode loop  0 step input action.item():  6\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n","\n","\n","\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 1. 2.]]\n","inner episode loop  0  state:  tensor([[0., 0., 0., 0., 2., 0., 1., 1., 2.]])\n","inner episode loop  0  probs:  tensor([[0.0866, 0.0707, 0.0569, 0.0633, 0.1626, 0.0905, 0.2574, 0.0821, 0.1298]],\n","       grad_fn=<SoftmaxBackward0>)\n","inner episode loop  0  action_probabilities:  Categorical(probs: torch.Size([1, 9]))\n","inner episode loop  0  action:  tensor([2])\n","\n","\n","\n","inner episode loop  0 step input action.item():  2\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 1. 2.]]\n","inner episode loop after step  0\n","step output state: \n"," [[0. 0. 1.]\n"," [2. 2. 0.]\n"," [1. 1. 2.]]\n","step output reward:  0\n","step output done:  0\n"]}]},{"cell_type":"code","source":["# prompt: offload a network policy using torch.save()\n","\n","# Save the trained policy network\n","#torch.save(policy.state_dict(), 'policy_network.pth')"],"metadata":{"id":"aeHViI_qo1VU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: load a torch model I saved from colab\n","\n","# import torch\n","# import torch.nn as nn\n","\n","# class PolicyNetwork(nn.Module):\n","#     def __init__(self):\n","#         super(PolicyNetwork, self).__init__()\n","#         self.fc = nn.Sequential(\n","#             nn.Linear(9, 512),\n","#             nn.ReLU(),\n","#             nn.Linear(512, 9),\n","#             nn.Softmax(dim=-1),\n","#         )\n","\n","#     def forward(self, x):\n","#         return self.fc(x)\n","\n","# # Load the saved model\n","# policy = PolicyNetwork()\n","# policy.load_state_dict(torch.load('policy_network.pth'))\n","# policy.eval() # Set the model to evaluation mode\n","\n","# # Now you can use the loaded policy\n","# # Example:\n","# # with torch.no_grad():\n","# #    state = torch.randn(1,9)  # Example input state\n","# #    probs = policy(state)\n","# #    print(probs)"],"metadata":{"id":"iLTgdhrYpglZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchviz import make_dot\n","\n","one_one = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","X = torch.FloatTensor(one_one)\n","\n","\n","y = policy(X)\n","print(y)\n","# make_dot(y.mean(), params=dict(policy.named_parameters()), show_attrs=True, show_saved=True)"],"metadata":{"id":"5XJqcPqv7S_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters_0 = policy.fc[0].weight.detach().numpy()\n","sns.heatmap(parameters_0, annot=True)\n","\n","\n"],"metadata":{"id":"XIYx4NvG9H3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters_2 = policy.fc[2].weight.detach().numpy()\n","sns.heatmap(parameters_2, annot=True)\n"],"metadata":{"id":"ytPd7Rj1_IPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cumulative_episode_rewards = []\n","cumulative_episodes_loss = []\n","for i in range(len(episode_rewards)):\n","  if i == 0:\n","    cumulative_episode_rewards.append(episode_rewards[i])\n","    cumulative_episodes_loss.append(episode_losses[i])\n","  else:\n","    cumulative_episode_rewards.append(cumulative_episode_rewards[i-1] + episode_rewards[i])\n","    cumulative_episodes_loss.append(cumulative_episodes_loss[i-1] - episode_losses[i])\n","    # if i > 400 and i < 500:\n","    #   print(i, \": \", cumulative_episode_rewards[i])\n","\n","\n","plt.plot(cumulative_episode_rewards)\n","plt.title('Training Reward Over Episodes')\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.show()\n","\n","\n","plt.plot(episode_losses)\n","plt.title('Loss Over Episodes')\n","plt.xlabel('Episode')\n","plt.ylabel('Loss')\n","plt.show()"],"metadata":{"id":"1_0tqEsCWw_8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["empty_board = np.zeros((3, 3))\n","\n","# Creates an empty board\n","def create_board():\n","    return(np.array([[0, 0, 0],\n","                     [0, 0, 0],\n","                     [0, 0, 0]]))\n","\n","def possibilities(board):\n","    l = []\n","    for i in range(len(board)):\n","        for j in range(len(board)):\n","\n","            if board[i][j] == 0:\n","                l.append((i, j))\n","    return(l)\n","\n","def network_best_move(board):\n","  state = board.flatten()\n","  state = torch.FloatTensor(state).unsqueeze(0)\n","  probs = policy(state)\n","  action_probabilities = Categorical(probs)\n","  action = torch.argmax(probs, dim=1)\n","  board[tensor_to_tuple[action.item()]] = 1\n","  return board\n","\n","def random_place(board):\n","    selection = possibilities(board)\n","    current_loc = random.choice(selection)\n","    board[current_loc] = 2\n","    return board\n","\n","def check_win(board):\n","    # Check rows\n","    for player in [1,2]:\n","      for i in range(3):\n","        if np.all(board[i, :] == player):\n","          return player\n","\n","      # Check columns\n","      for j in range(3):\n","        if np.all(board[:, j] == player):\n","          return player\n","\n","      #Check diagonal\n","      if np.all(np.diag(board) == player):\n","        return player\n","      if np.all(np.diag(np.fliplr(board)) == player):\n","        return player\n","\n","    #Check tie\n","    if np.all(board != 0):\n","      return -1\n","\n","    return 0\n","\n","\n"],"metadata":{"id":"08Cg5ChHG2pl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Experiments to verify Deep Q learning actually works\n","#Testing agent against random moves, ideally the agent would win ~99% of games\n","#Is the win percentage affected if agent is player 1 or player 2?\n","\n","#function for agent as player 1\n","def player1_agent_vs_random_moves():\n","\n","  board, winner, counter = create_board(), 0, 1\n","  print(board)\n","\n","  player1 = 1\n","  player2 = 2\n","  while winner == 0:\n","      for player in [player1, player2]:\n","        if player == 1:\n","          board = network_best_move(board)\n","          print(\"Board after \" + str(counter) + \" move\")\n","          print(board)\n","          counter += 1\n","\n","        if player == 2:\n","          board = random_place(board)\n","          print(\"Board after \" + str(counter) + \" move\")\n","          print(board)\n","          counter += 1\n","\n","        if counter > 4:\n","            winner = check_win(board)\n","        if winner != 0:\n","            break\n","  print(\"Winner: \", winner)\n","  return winner"],"metadata":{"id":"IWjtiA1N6Chn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 100000\n","agent_win_counter = 0\n","agent_loss_counter = 0\n","agent_tie_counter = 0\n","\n","start_player_1_agent = time.time()\n","for i in range(n):\n","  winner = player1_agent_vs_random_moves()\n","  if winner == 1:\n","    agent_win_counter += 1\n","  elif winner == 2:\n","    agent_loss_counter += 1\n","  else:\n","    agent_tie_counter += 1\n","end_player_1_agent = time.time()\n","\n","print(\"Trials took \" + str(end_player_1_agent-start_player_1_agent) + \" seconds\")\n","print(str(agent_win_counter) + \" wins out of \" + str(n) + \" trials\")\n","\n","print(\"win percentage: \", (agent_win_counter/n)*100,\"%\")\n","print(\"tie percentage: \", (agent_tie_counter/n)*100,\"%\")\n","print(\"lose percentage: \", (agent_loss_counter/n)*100,\"%\")"],"metadata":{"id":"2l_OTngt5762"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"win percentage: \", (agent_win_counter/n)*100,\"%\")\n","print(\"tie percentage: \", (agent_tie_counter/n)*100,\"%\")\n","print(\"lose percentage: \", (agent_loss_counter/n)*100,\"%\")"],"metadata":{"id":"LxAa0rf0DGnV"},"execution_count":null,"outputs":[]}]}
