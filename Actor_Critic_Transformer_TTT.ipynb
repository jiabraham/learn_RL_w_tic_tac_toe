{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31886,"status":"ok","timestamp":1739765036300,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"},"user_tz":300},"id":"zrmvdi6vaaKw","outputId":"050ee4f3-882c-471b-893d-03251d42ebcc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","import graphviz\n","import seaborn as sns\n","import time\n","import math\n","from torch.nn.utils import clip_grad_norm_\n","from collections import deque\n","import torch.nn.functional as F\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from google.colab import files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"executionInfo":{"elapsed":1100,"status":"error","timestamp":1739750683788,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"},"user_tz":300},"id":"kRHMf0esbDVy","outputId":"b3e72b51-c14f-4b47-88ac-5939dc418099"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/nn_training_data_first_half.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5b46c8545866>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/nn_training_data_third_half.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moffline_TTT_trajectories_first_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moffline_TTT_trajectories_second_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moffline_TTT_trajectories_third_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/nn_training_data_first_half.csv'"]}],"source":["path_1 = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/nn_training_data_first_half.csv\"\n","path_2 = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/nn_training_data_second_half.csv\"\n","path_3 = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/nn_training_data_third_half.csv\"\n","\n","offline_TTT_trajectories_first_half = pd.read_csv(path_1).to_numpy()\n","offline_TTT_trajectories_second_half = pd.read_csv(path_2).to_numpy()\n","offline_TTT_trajectories_third_half = pd.read_csv(path_3).to_numpy()\n","\n","offline_TTT_trajectories = np.concatenate((offline_TTT_trajectories_first_half, offline_TTT_trajectories_second_half, offline_TTT_trajectories_third_half), axis=0)\n","#1.75 million games\n","print(offline_TTT_trajectories.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1045,"status":"ok","timestamp":1739765041032,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"},"user_tz":300},"id":"5ikZ6Tzf4TEC","outputId":"656d50ac-1a4d-4b83-9bec-f2f641376a13"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: (0, 0), 1: (0, 1), 2: (0, 2), 3: (1, 0), 4: (1, 1), 5: (1, 2), 6: (2, 0), 7: (2, 1), 8: (2, 2)}\n"]}],"source":["# 1. Define your environment\n","class TicTacToe:\n","  def __init__(self):\n","    self.board = np.zeros((3, 3))\n","    self.one_move_back = np.zeros((3, 3))\n","    self.two_moves_back = np.zeros((3, 3))\n","\n","  def reset(self):\n","    self.board = np.zeros((3, 3))\n","    self.one_move_back = np.zeros((3, 3))\n","    self.two_moves_back = np.zeros((3, 3))\n","    return [self.two_moves_back, self.one_move_back, self.board]\n","\n","  def check_win(self, player):\n","    # Check rows\n","    for i in range(3):\n","      if np.all(self.board[i, :] == player):\n","        return player\n","\n","    # Check columns\n","    for j in range(3):\n","      if np.all(self.board[:, j] == player):\n","        return player\n","\n","    #Check diagonal\n","    if np.all(np.diag(self.board) == player):\n","      return player\n","    if np.all(np.diag(np.fliplr(self.board)) == player):\n","      return player\n","\n","    #Check tie\n","    if np.all(self.board != 0):\n","      return -1\n","\n","    return 0\n","\n","  # Check for empty places on board\n","  def possibilities(self):\n","    l = []\n","    for i in range(len(self.board)):\n","        for j in range(len(self.board)):\n","\n","            if self.board[i][j] == 0:\n","                l.append((i, j))\n","    return(l)\n","\n","  #training\n","  def random_step(self, player):\n","    selection = self.possibilities()\n","    current_loc = random.choice(selection)\n","    self.two_moves_back = self.one_move_back.copy()\n","    self.one_move_back = self.board.copy()\n","    self.board[current_loc] = player\n","    done = self.check_win(player)\n","    reward = 0\n","    if done == 2:\n","      reward = -1.5\n","    state = np.concatenate((self.two_moves_back, self.one_move_back, self.board), axis=0)\n","    return state, reward, done\n","\n","  def network_step(self, action, player):\n","    #print(action)\n","    self.two_moves_back = self.one_move_back.copy()\n","    self.one_move_back = self.board.copy()\n","    row, col = action\n","    self.board[row, col] = player\n","    done = self.check_win(player)\n","    reward = 0\n","    if done == 1:\n","      reward = 1\n","    elif done == -1:\n","      reward = 0.1\n","    state = np.concatenate((self.two_moves_back, self.one_move_back, self.board), axis=0)\n","    return state, reward, done\n","\n","#takes a 2-d numpy array and create a string representation\n","def numpy_array_to_string(array):\n","  string_array = \"\"\n","  for row in array:\n","    for element in row:\n","      string_array += str(element)\n","  return string_array\n","tensor_to_tuple = {}\n","for i in range(9):\n","  tensor_to_tuple[i] = (i//3, i%3)\n","print(tensor_to_tuple)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhdM6eGt5Tob"},"outputs":[],"source":["#Original Code\n","# Define the Transformer model for the policy and advantage functions\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dim, n_layers, n_heads, dropout=0.1):\n","        super(TransformerModel, self).__init__()\n","\n","        self.embedding = nn.Linear(input_dim, hidden_dim)  # Linear embedding layer\n","        self.transformer = nn.Transformer(\n","            d_model=hidden_dim,\n","            nhead=n_heads,\n","            num_encoder_layers=n_layers,\n","            num_decoder_layers=n_layers,  # Can be 0 if not using decoder\n","            dropout=dropout,\n","            batch_first=True # Important for handling sequences\n","        )\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        # For a simple sequence input, we can use a placeholder target sequence\n","        # or even pass the input sequence as the target.  Adjust as needed\n","        # for your specific problem.\n","        seq_len = x.size(1) # Get sequence length\n","        tgt = torch.zeros_like(embedded) # Placeholder target sequence\n","        # OR, use input as target for auto-regressive style:\n","        #tgt = embedded\n","\n","        output = self.transformer(embedded, tgt)  # Pass through transformer\n","        output = self.fc(output[:, -1, :]) # Get the last timestep's output (adjust if needed)\n","        return output\n","\n","\n","# Define the Advantage Actor-Critic (A2C) agent\n","class A2CAgent:\n","    def __init__(self, input_dim, action_dim, hidden_dim=256, n_layers=6, n_heads=8, dropout=0.1, lr=1e-5, gamma=0.99):\n","        self.actor = TransformerModel(input_dim, action_dim, hidden_dim, n_layers, n_heads, dropout)\n","        self.critic = TransformerModel(input_dim, 1, hidden_dim, n_layers, n_heads, dropout) # Critic outputs a single value\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n","        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n","        self.gamma = gamma\n","\n","    def act(self, state):\n","        state = state.flatten()\n","        current_board = state[18:27].copy().reshape(3, 3)\n","\n","        action_mask = np.zeros((3, 3))\n","        # Create action mask so sample produces a valid state prediction\n","        for i in range(len(action_mask)):\n","          for j in range(len(action_mask)):\n","            if current_board[(i,j)] == 0:\n","              action_mask[i][j] = 1\n","        action_mask = torch.FloatTensor(action_mask.flatten()) # Move action_mask to device\n","\n","        state = torch.tensor(state, dtype=torch.float).unsqueeze(0) # Add batch dimension\n","        action_probs = F.softmax(self.actor(state), dim=-1)\n","        action = torch.multinomial(action_probs, 1).item()\n","        return action\n","\n","    def train(self, states, actions, rewards, next_states, dones):\n","        states = torch.tensor(np.array(states), dtype=torch.float)\n","        actions = torch.tensor(actions, dtype=torch.long)\n","        rewards = torch.tensor(rewards, dtype=torch.float)\n","        next_states = torch.tensor(np.array(next_states), dtype=torch.float)\n","        dones = torch.tensor(dones, dtype=torch.float)\n","\n","        # Calculate TD target\n","        values = self.critic(states).squeeze(1) # Get value predictions\n","        next_values = self.critic(next_states).squeeze(1)\n","        td_target = rewards + (1 - dones) * self.gamma * next_values\n","        advantage = td_target - values\n","\n","        # Actor loss\n","        action_probs = F.softmax(self.actor(states), dim=-1)\n","        log_probs = F.log_softmax(self.actor(states), dim=-1)\n","        actor_loss = -(log_probs[torch.arange(states.size(0)), actions] * advantage).mean()\n","\n","        # Critic loss (MSE)\n","        critic_loss = advantage.pow(2).mean()\n","\n","        # Update networks\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        return actor_loss.item(), critic_loss.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fPW3u1PnkV5"},"outputs":[],"source":["#Modified transformer for tic tac toe\n","# Define the Transformer model for the policy and advantage functions\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dim, n_layers, n_heads, dropout=0.1):\n","        super(TransformerModel, self).__init__()\n","\n","        self.embedding = nn.Linear(input_dim, hidden_dim)  # Linear embedding layer\n","        self.transformer = nn.Transformer(\n","            d_model=hidden_dim,\n","            nhead=n_heads,\n","            num_encoder_layers=n_layers,\n","            num_decoder_layers=n_layers,  # Can be 0 if not using decoder\n","            dropout=dropout,\n","            batch_first=True # Important for handling sequences\n","        )\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        # For a simple sequence input, we can use a placeholder target sequence\n","        # or even pass the input sequence as the target.  Adjust as needed\n","        # for your specific problem.\n","        seq_len = x.size(1) # Get sequence length\n","        tgt = torch.zeros_like(embedded) # Placeholder target sequence\n","        # OR, use input as target for auto-regressive style:\n","        #tgt = embedded\n","        output = self.transformer(embedded, tgt)\n","        #print(\"output: \", output)\n","        #output = output[:, -1]\n","        #print(\"output after last column: \", output)\n","\n","        #output = self.transformer(embedded, tgt)  # Pass through transformer\n","        output = self.fc(output) # Get the last timestep's output (adjust if needed)\n","        #print(\"output after fc: \", output)\n","        return output\n","\n","\n","# Define the Advantage Actor-Critic (A2C) agent\n","class A2CAgent:\n","    def __init__(self, input_dim, action_dim, hidden_dim=128, n_layers=6, n_heads=8, dropout=0.1, lr=1e-5, gamma=0.99):\n","        self.actor = TransformerModel(input_dim, action_dim, hidden_dim, n_layers, n_heads, dropout)\n","        self.critic = TransformerModel(input_dim, 1, hidden_dim, n_layers, n_heads, dropout) # Critic outputs a single value\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n","        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n","        self.gamma = gamma\n","\n","    def act(self, state):\n","        state = state.flatten()\n","        current_board = state[18:27].copy().reshape(3, 3)\n","\n","        action_mask = np.zeros((3, 3))\n","        # Create action mask so sample produces a valid state prediction\n","        for i in range(len(action_mask)):\n","          for j in range(len(action_mask)):\n","            if current_board[(i,j)] == 0:\n","              action_mask[i][j] = 1\n","        action_mask = torch.FloatTensor(action_mask.flatten()) # Move action_mask to device\n","\n","        state = torch.tensor(state, dtype=torch.float).unsqueeze(0) # Add batch dimension\n","        probs = F.softmax(self.actor(state), dim=-1)\n","        masked_probs = probs * action_mask\n","        masked_probs = masked_probs / masked_probs.sum()\n","        action = torch.multinomial(masked_probs, 1).item()\n","        return action\n","\n","    def train_actor_critic(self, states, actions, rewards, next_states, dones):\n","        print(\"rewards: \", rewards)\n","        states = torch.tensor(np.array(states), dtype=torch.float)\n","        actions = torch.tensor(actions, dtype=torch.long)\n","        rewards = torch.tensor(rewards, dtype=torch.float)\n","        next_states = torch.tensor(np.array(next_states), dtype=torch.float)\n","        dones = torch.tensor(dones, dtype=torch.float)\n","\n","        # Calculate TD target\n","        critic_states = states.detach()\n","        critic_next_states = next_states.detach()\n","        values = self.critic(critic_states).squeeze(1) # Get value predictions\n","        print(\"values: \", values)\n","        next_values = self.critic(critic_next_states).squeeze(1)\n","        print(\"next values: \", next_values)\n","        #values = self.critic(states).squeeze(1) # Get value predictions\n","        #next_values = self.critic(next_states).squeeze(1)\n","        td_target = rewards + (1 - dones) * self.gamma * next_values\n","\n","        print(\"td target: \", td_target)\n","        advantage = td_target - values\n","        print(\"advantage: \", advantage)\n","        # Actor loss\n","        action_probs = F.softmax(self.actor(states), dim=-1)\n","        log_probs = F.log_softmax(self.actor(states), dim=-1)\n","        detached_advantage = advantage.detach()\n","        actor_loss = -(log_probs[torch.arange(states.size(0)), actions] * detached_advantage).mean()\n","        #actor_loss = -(log_probs[torch.arange(states.size(0)), actions] * advantage).mean()\n","\n","\n","        # Critic loss (MSE)\n","        critic_loss = advantage.pow(2).mean()\n","        #critic_loss = advantage.pow(2).mean()\n","\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Update networks\n","        # self.actor_optimizer.zero_grad()\n","        # actor_loss.backward()\n","        # self.actor_optimizer.step()\n","\n","        return actor_loss.item(), critic_loss.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCs8ezqLjphG"},"outputs":[],"source":["# Example training loop\n","def train(env, agent, num_episodes=500000):\n","  for episode in range(num_episodes):\n","    state = [entry.flatten() for entry in env.reset()]\n","    state = np.concatenate((state[0], state[1], state[2]), axis=None)\n","\n","    done = 0\n","    states = []\n","    next_states = []\n","    actions = []\n","    rewards = []\n","    dones = []\n","    move_counter = 0\n","\n","    state = state.flatten()\n","    current_board = state[18:27].copy().reshape(3, 3)\n","    print(\"board after \" + str(move_counter) + \" moves\")\n","    print(current_board)\n","\n","    #Intra-trajectory loop\n","    while done == 0:\n","      state = state.flatten()\n","      current_board = state[18:27].copy().reshape(3, 3)\n","\n","      action_mask = np.zeros((3, 3))\n","      # Create action mask so sample produces a valid state prediction\n","      for i in range(len(action_mask)):\n","        for j in range(len(action_mask)):\n","          if current_board[(i,j)] == 0:\n","            action_mask[i][j] = 1\n","      action_mask = torch.FloatTensor(action_mask.flatten()) # Move action_mask to device\n","\n","      action = agent.act(state)\n","\n","      states.append(state)\n","      actions.append(action)\n","\n","      state, reward, done = env.network_step(tensor_to_tuple[action], player = 1)\n","      move_counter += 1\n","      print(\"board after \" + str(move_counter) + \" moves\")\n","      state = state.flatten()\n","      print(state[18:27].copy().reshape(3, 3))\n","      next_states.append(state)\n","\n","\n","      #Player two move if nonterminal state\n","      if done == 0:\n","        state, reward, done = env.random_step(player = 2)\n","        move_counter += 1\n","        print(\"board after \" + str(move_counter) + \" moves\")\n","        state = state.flatten()\n","        print(state[18:27].copy().reshape(3, 3))\n","\n","      rewards.append(reward)\n","      dones.append(done)\n","\n","    episode_rewards.append(reward)\n","    # print(\"states:\")\n","    # for state in states:\n","    #   print(state)\n","    # print(\"actions: \\n\", actions)\n","    # print(\"rewards: \\n\", rewards)\n","    # print(\"next_states: \\n\", next_states)\n","    # print(\"dones: \", dones)\n","    # break\n","\n","    actor_loss, critic_loss = agent.train_actor_critic(states, actions, rewards, next_states, dones)\n","    print(f\"Episode: {episode}, Total Reward: {sum(episode_rewards)}, Actor Loss: {actor_loss}, Critic Loss: {critic_loss}\")\n","    if episode % 50000 == 0:\n","      print(\"saved model: \", episode)\n","      # Save the trained policy network\n","      torch.save(agent.actor.state_dict(), '/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/actor critic networks/actor_network_' + str(episode/50000)+'.pth')\n","      torch.save(agent.critic.state_dict(), '/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Transformers/actor critic networks/critic_network_' + str(episode/50000)+'.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BqkgMzcIp6RW","outputId":"4dea5348-f6bb-4981-9dc1-380edc32066c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","[[1. 1. 0.]\n"," [2. 0. 2.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[1. 1. 0.]\n"," [2. 2. 2.]\n"," [1. 0. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.0144, -0.0060, -0.1079], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2323, -0.1147, -0.0778], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2300, -0.1135, -0.9230], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2156, -0.1075, -0.8151], grad_fn=<SubBackward0>)\n","Episode: 259729, Total Reward: 77874.80000000258, Actor Loss: -0.688973605632782, Critic Loss: 0.24079012870788574\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [0. 1. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [1. 1. 0.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [1. 1. 2.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[0. 2. 1.]\n"," [1. 1. 2.]\n"," [0. 1. 2.]]\n","board after 8 moves\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [0. 1. 2.]]\n","board after 9 moves\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [1. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5696, 0.6743, 0.6051, 0.6118, 0.5743], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5478, 0.5366, 0.5053, 0.4971, 0.5407], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5424, 0.5312, 0.5003, 0.4922, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0272, -0.1431, -0.1048, -0.1196,  0.4257], grad_fn=<SubBackward0>)\n","Episode: 259730, Total Reward: 77875.80000000258, Actor Loss: -0.0604199543595314, Critic Loss: 0.04554805904626846\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[0. 1. 0.]\n"," [2. 2. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[0. 1. 0.]\n"," [2. 2. 0.]\n"," [1. 1. 0.]]\n","board after 6 moves\n","[[0. 1. 0.]\n"," [2. 2. 2.]\n"," [1. 1. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.2474, -0.2975, -0.2475], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3343, -0.2894, -0.3259], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3309, -0.2865, -0.6774], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0836,  0.0110, -0.4298], grad_fn=<SubBackward0>)\n","Episode: 259731, Total Reward: 77874.80000000258, Actor Loss: -0.42889824509620667, Critic Loss: 0.06394930183887482\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 2.]]\n","board after 5 moves\n","[[1. 1. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 2.]]\n","board after 6 moves\n","[[1. 1. 0.]\n"," [0. 0. 1.]\n"," [2. 2. 2.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.2095, -0.1772, -0.0805], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2491, -0.2835, -0.3370], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2466, -0.2807, -0.6663], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0370, -0.1035, -0.5858], grad_fn=<SubBackward0>)\n","Episode: 259732, Total Reward: 77873.80000000258, Actor Loss: -0.47241973876953125, Critic Loss: 0.11842107772827148\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [1. 2. 0.]\n"," [0. 1. 0.]]\n","board after 5 moves\n","[[0. 0. 2.]\n"," [1. 2. 0.]\n"," [0. 1. 1.]]\n","board after 6 moves\n","[[2. 0. 2.]\n"," [1. 2. 0.]\n"," [0. 1. 1.]]\n","board after 7 moves\n","[[2. 0. 2.]\n"," [1. 2. 1.]\n"," [0. 1. 1.]]\n","board after 8 moves\n","[[2. 2. 2.]\n"," [1. 2. 1.]\n"," [0. 1. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.2694, -0.2988, -0.3228, -0.3610], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4262, -0.3459, -0.2856, -0.3401], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4219, -0.3424, -0.2828, -0.6633], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1525, -0.0437,  0.0400, -0.3023], grad_fn=<SubBackward0>)\n","Episode: 259733, Total Reward: 77872.80000000258, Actor Loss: -0.27327316999435425, Critic Loss: 0.029535163193941116\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[2. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[2. 0. 1.]\n"," [1. 0. 2.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [1. 2. 2.]\n"," [1. 0. 0.]]\n","board after 7 moves\n","[[2. 0. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","board after 8 moves\n","[[2. 0. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.0832, -0.0851, -0.0043, -0.0462], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0884, -0.1400, -0.0719, -0.1530], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0875, -0.1386, -0.0712, -0.8485], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0044, -0.0535, -0.0669, -0.8023], grad_fn=<SubBackward0>)\n","Episode: 259734, Total Reward: 77871.80000000258, Actor Loss: -0.6704905033111572, Critic Loss: 0.16274487972259521\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 2. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [2. 2. 0.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [2. 2. 0.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[1. 0. 1.]\n"," [2. 2. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([0.0745, 0.1145, 0.0301, 0.0306], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([ 0.0147, -0.0379, -0.0238, -0.0320], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([ 0.0146, -0.0376, -0.0236, -0.9683], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0599, -0.1521, -0.0537, -0.9989], grad_fn=<SubBackward0>)\n","Episode: 259735, Total Reward: 77870.80000000258, Actor Loss: -0.6268970966339111, Critic Loss: 0.25685781240463257\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[2. 0. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[2. 1. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[2. 1. 2.]\n"," [1. 0. 0.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[2. 1. 2.]\n"," [1. 0. 1.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [1. 0. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6198, 0.6269, 0.6028, 0.6166, 0.6115], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5067, 0.4047, 0.5719, 0.5453, 0.5070], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5016, 0.4007, 0.5661, 0.5398, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1182, -0.2262, -0.0367, -0.0768,  0.3885], grad_fn=<SubBackward0>)\n","Episode: 259736, Total Reward: 77871.80000000258, Actor Loss: 0.02033204399049282, Critic Loss: 0.0446632094681263\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [2. 0. 0.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[1. 1. 2.]\n"," [2. 0. 0.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[1. 1. 2.]\n"," [2. 2. 0.]\n"," [0. 0. 1.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [2. 2. 0.]\n"," [1. 0. 1.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [2. 2. 2.]\n"," [1. 0. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.1275, -0.1337, -0.1528, -0.1617], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2583, -0.3134, -0.3165, -0.3189], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2557, -0.3103, -0.3134, -0.6843], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1282, -0.1766, -0.1605, -0.5226], grad_fn=<SubBackward0>)\n","Episode: 259737, Total Reward: 77870.80000000258, Actor Loss: -0.5782809257507324, Critic Loss: 0.08662594854831696\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[1. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 2. 1.]\n"," [1. 0. 0.]\n"," [0. 2. 0.]]\n","board after 6 moves\n","[[1. 2. 1.]\n"," [1. 0. 0.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [1. 0. 0.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0553,  0.0447,  0.0070,  0.0187], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1314, -0.2113, -0.1888, -0.2265], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1301, -0.2092, -0.1869,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0748, -0.2539, -0.1940,  0.9813], grad_fn=<SubBackward0>)\n","Episode: 259738, Total Reward: 77871.80000000258, Actor Loss: 0.36371707916259766, Critic Loss: 0.267659455537796\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[2. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 1. 0.]]\n","board after 5 moves\n","[[2. 0. 0.]\n"," [1. 2. 0.]\n"," [1. 1. 0.]]\n","board after 6 moves\n","[[2. 0. 2.]\n"," [1. 2. 0.]\n"," [1. 1. 0.]]\n","board after 7 moves\n","[[2. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 1. 0.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.4246, 0.4432, 0.3958, 0.5169, 0.4084], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.3217, 0.3073, 0.4328, 0.3803, 0.3059], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.3184, 0.3042, 0.4285, 0.3765, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1061, -0.1390,  0.0327, -0.1404,  0.5916], grad_fn=<SubBackward0>)\n","Episode: 259739, Total Reward: 77872.80000000258, Actor Loss: 0.08205743134021759, Critic Loss: 0.08027750253677368\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[2. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[2. 0. 0.]\n"," [1. 1. 2.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[2. 0. 0.]\n"," [1. 1. 2.]\n"," [0. 2. 1.]]\n","board after 7 moves\n","[[2. 0. 1.]\n"," [1. 1. 2.]\n"," [0. 2. 1.]]\n","board after 8 moves\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [0. 2. 1.]]\n","board after 9 moves\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6094, 0.6315, 0.7139, 0.6606, 0.6084], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5025, 0.5679, 0.5504, 0.5109, 0.5574], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4975, 0.5623, 0.5449, 0.5058, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1119, -0.0693, -0.1690, -0.1547,  0.3916], grad_fn=<SubBackward0>)\n","Episode: 259740, Total Reward: 77873.80000000258, Actor Loss: -0.06789936125278473, Critic Loss: 0.04463038593530655\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [1. 2. 0.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 2. 0.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 2. 1.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [0. 2. 1.]\n"," [1. 2. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([ 0.0176, -0.0203, -0.0721], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1638, -0.2410, -0.2252], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1621, -0.2386, -0.7771], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1797, -0.2182, -0.7050], grad_fn=<SubBackward0>)\n","Episode: 259741, Total Reward: 77872.80000000258, Actor Loss: -0.816875696182251, Critic Loss: 0.19229333102703094\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 1.]\n"," [0. 2. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 1.]\n"," [0. 2. 2.]\n"," [0. 1. 0.]]\n","board after 6 moves\n","[[1. 0. 1.]\n"," [0. 2. 2.]\n"," [2. 1. 0.]]\n","board after 7 moves\n","[[1. 1. 1.]\n"," [0. 2. 2.]\n"," [2. 1. 0.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([ 0.0080, -0.0018, -0.0122, -0.0139], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2339, -0.1667, -0.1777, -0.1731], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2316, -0.1651, -0.1759,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2396, -0.1633, -0.1637,  1.0139], grad_fn=<SubBackward0>)\n","Episode: 259742, Total Reward: 77873.80000000258, Actor Loss: 0.20610183477401733, Critic Loss: 0.2847130298614502\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [0. 1. 2.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [2. 1. 2.]\n"," [1. 0. 0.]]\n","board after 7 moves\n","[[1. 0. 2.]\n"," [2. 1. 2.]\n"," [1. 0. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0150, -0.0098, -0.0327, -0.0859], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.0564, 0.0269, 0.0334, 0.0792], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.0558, 0.0266, 0.0331, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([0.0708, 0.0365, 0.0658, 1.0859], grad_fn=<SubBackward0>)\n","Episode: 259743, Total Reward: 77874.80000000258, Actor Loss: 0.5640237331390381, Critic Loss: 0.2974548041820526\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [0. 2. 0.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.3044, -0.1967, -0.1724], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3432, -0.3408, -0.4082], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3398, -0.3374, -0.5959], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0354, -0.1406, -0.4235], grad_fn=<SubBackward0>)\n","Episode: 259744, Total Reward: 77873.80000000258, Actor Loss: -0.41050049662590027, Critic Loss: 0.0667882189154625\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [0. 2. 0.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([0.0032, 0.1043, 0.0797], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2954, -0.1765, -0.2560], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2924, -0.1748, -0.7466], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2956, -0.2791, -0.8262], grad_fn=<SubBackward0>)\n","Episode: 259745, Total Reward: 77872.80000000258, Actor Loss: -1.0832878351211548, Critic Loss: 0.2826351225376129\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [2. 1. 0.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [2. 1. 0.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [2. 1. 0.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[1. 1. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.5276, 0.5197, 0.6004, 0.5077, 0.4567], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5697, 0.6044, 0.5436, 0.5904, 0.5929], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5640, 0.5983, 0.5382, 0.5845, 1.2739], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0365,  0.0786, -0.0622,  0.0768,  0.8172], grad_fn=<SubBackward0>)\n","Episode: 259746, Total Reward: 77872.90000000258, Actor Loss: 0.3319048285484314, Critic Loss: 0.13700437545776367\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 2.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 2. 2.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [1. 2. 2.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [1. 0. 0.]\n"," [1. 2. 2.]]\n","board after 7 moves\n","[[2. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 2. 2.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [1. 0. 2.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [1. 1. 2.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5995, 0.6040, 0.6568, 0.6697, 0.6030], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6062, 0.5351, 0.5965, 0.5963, 0.5942], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6002, 0.5297, 0.5905, 0.5904, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0007, -0.0743, -0.0663, -0.0793,  0.3970], grad_fn=<SubBackward0>)\n","Episode: 259747, Total Reward: 77873.90000000258, Actor Loss: 0.08792048692703247, Critic Loss: 0.03477020561695099\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 2. 1.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[1. 2. 1.]\n"," [0. 2. 2.]\n"," [1. 0. 0.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [0. 2. 2.]\n"," [1. 1. 0.]]\n","board after 8 moves\n","[[1. 2. 1.]\n"," [0. 2. 2.]\n"," [1. 1. 2.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.4886, 0.4604, 0.4877, 0.4857, 0.4676], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4697, 0.4169, 0.3924, 0.4177, 0.4274], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4650, 0.4128, 0.3885, 0.4135, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0236, -0.0476, -0.0992, -0.0722,  0.5324], grad_fn=<SubBackward0>)\n","Episode: 259748, Total Reward: 77874.90000000258, Actor Loss: 0.1293140947818756, Critic Loss: 0.060260236263275146\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[1. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 6 moves\n","[[1. 2. 1.]\n"," [0. 0. 0.]\n"," [2. 2. 1.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [0. 0. 1.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0171, -0.0317, -0.0205,  0.0139], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.1338, 0.1905, 0.2673, 0.1234], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.1324, 0.1886, 0.2646, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([0.1495, 0.2203, 0.2851, 0.9861], grad_fn=<SubBackward0>)\n","Episode: 259749, Total Reward: 77875.90000000258, Actor Loss: 0.8721880912780762, Critic Loss: 0.2811432480812073\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 4 moves\n","[[1. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 5 moves\n","[[1. 2. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 2.]]\n","board after 6 moves\n","[[1. 2. 0.]\n"," [0. 1. 0.]\n"," [1. 2. 2.]]\n","board after 7 moves\n","[[1. 2. 0.]\n"," [1. 1. 0.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0179, 0.1293, 0.0431, 0.0179], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1357, -0.1510, -0.1896, -0.1469], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1343, -0.1495, -0.1877,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1522, -0.2788, -0.2309,  0.9821], grad_fn=<SubBackward0>)\n","Episode: 259750, Total Reward: 77876.90000000258, Actor Loss: 0.24314182996749878, Critic Loss: 0.279678612947464\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 2. 0.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 2. 0.]\n"," [1. 0. 1.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [2. 2. 0.]\n"," [1. 0. 1.]]\n","board after 7 moves\n","[[1. 0. 2.]\n"," [2. 2. 0.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.3117, -0.3494, -0.2751, -0.2667], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4051, -0.2995, -0.3789, -0.3717], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4011, -0.2965, -0.3751,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0894,  0.0530, -0.1000,  1.2667], grad_fn=<SubBackward0>)\n","Episode: 259751, Total Reward: 77877.90000000258, Actor Loss: 0.7913613319396973, Critic Loss: 0.4063583314418793\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[2. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[2. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[2. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[2. 1. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[2. 1. 2.]\n"," [2. 0. 1.]\n"," [0. 0. 1.]]\n","board after 7 moves\n","[[2. 1. 2.]\n"," [2. 1. 1.]\n"," [0. 0. 1.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [2. 1. 1.]\n"," [2. 0. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.1438, -0.2669, -0.1560, -0.1663], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3998, -0.3525, -0.3838, -0.4164], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3958, -0.3489, -0.3800, -0.5877], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2520, -0.0821, -0.2239, -0.4214], grad_fn=<SubBackward0>)\n","Episode: 259752, Total Reward: 77876.90000000258, Actor Loss: -0.6502909660339355, Critic Loss: 0.0744955986738205\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 1. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [2. 1. 1.]]\n","board after 5 moves\n","[[0. 0. 0.]\n"," [0. 2. 1.]\n"," [2. 1. 1.]]\n","board after 6 moves\n","[[0. 0. 2.]\n"," [0. 2. 1.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.3908, -0.3979, -0.3759], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4801, -0.4813, -0.5439], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4753, -0.4764, -0.4616], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0845, -0.0785, -0.0856], grad_fn=<SubBackward0>)\n","Episode: 259753, Total Reward: 77875.90000000258, Actor Loss: -0.18356233835220337, Critic Loss: 0.00687843794003129\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 1.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 1.]\n"," [2. 0. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 1. 1.]\n"," [2. 0. 0.]\n"," [0. 2. 0.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([0.0582, 0.0611, 0.1013], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1349, -0.1395, -0.1147], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1336, -0.1381,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1918, -0.1992,  0.8987], grad_fn=<SubBackward0>)\n","Episode: 259754, Total Reward: 77876.90000000258, Actor Loss: 0.3046867251396179, Critic Loss: 0.2947221100330353\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [2. 1. 0.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [0. 0. 1.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [0. 2. 1.]\n"," [2. 1. 0.]]\n","board after 7 moves\n","[[0. 2. 1.]\n"," [1. 2. 1.]\n"," [2. 1. 0.]]\n","board after 8 moves\n","[[0. 2. 1.]\n"," [1. 2. 1.]\n"," [2. 1. 2.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [1. 2. 1.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.6239, 0.6197, 0.6777, 0.6176, 0.6939], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5018, 0.5071, 0.5569, 0.5608, 0.5555], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4968, 0.5020, 0.5513, 0.5552, 1.2000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1271, -0.1177, -0.1264, -0.0624,  0.5061], grad_fn=<SubBackward0>)\n","Episode: 259755, Total Reward: 77877.00000000259, Actor Loss: 0.037308625876903534, Critic Loss: 0.061206478625535965\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 1. 2.]]\n","board after 6 moves\n","[[2. 2. 1.]\n"," [0. 0. 0.]\n"," [1. 1. 2.]]\n","board after 7 moves\n","[[2. 2. 1.]\n"," [0. 0. 1.]\n"," [1. 1. 2.]]\n","board after 8 moves\n","[[2. 2. 1.]\n"," [2. 0. 1.]\n"," [1. 1. 2.]]\n","board after 9 moves\n","[[2. 2. 1.]\n"," [2. 1. 1.]\n"," [1. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6231, 0.7103, 0.7201, 0.7448, 0.6328], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5769, 0.5283, 0.6324, 0.5838, 0.6268], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5712, 0.5230, 0.6261, 0.5779, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0519, -0.1874, -0.0940, -0.1668,  0.3672], grad_fn=<SubBackward0>)\n","Episode: 259756, Total Reward: 77878.00000000259, Actor Loss: -0.04177938774228096, Critic Loss: 0.041863229125738144\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 2. 1.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [0. 2. 1.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[0. 0. 2.]\n"," [1. 2. 1.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[0. 0. 2.]\n"," [1. 2. 1.]\n"," [2. 0. 1.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.4219, -0.3872, -0.3342], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4256, -0.4433, -0.3243], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4213, -0.4389, -0.6789], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0006, -0.0516, -0.3447], grad_fn=<SubBackward0>)\n","Episode: 259757, Total Reward: 77877.00000000259, Actor Loss: -0.30115455389022827, Critic Loss: 0.04049687832593918\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 2. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 2. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 2. 0.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[1. 2. 0.]\n"," [2. 2. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([0.1485, 0.1158, 0.1187, 0.1335], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0622, -0.0547, -0.0559, -0.0117], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0616, -0.0542, -0.0554, -0.9885], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2101, -0.1700, -0.1740, -1.1220], grad_fn=<SubBackward0>)\n","Episode: 259758, Total Reward: 77876.00000000259, Actor Loss: -0.9715936183929443, Critic Loss: 0.3405282497406006\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 2.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[0. 2. 2.]\n"," [1. 1. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[0. 2. 2.]\n"," [1. 1. 0.]\n"," [1. 0. 2.]]\n","board after 7 moves\n","[[1. 2. 2.]\n"," [1. 1. 0.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0025, 0.0244, 0.0169, 0.0898], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0449, -0.0651, -0.0319, -0.0692], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0445, -0.0644, -0.0316,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0470, -0.0888, -0.0484,  0.9102], grad_fn=<SubBackward0>)\n","Episode: 259759, Total Reward: 77877.00000000259, Actor Loss: 0.2706114649772644, Critic Loss: 0.2102179229259491\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 2.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 2.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 1. 2.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 2.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.1933, -0.1195, -0.1141], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2811, -0.2475, -0.2867], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2783, -0.2451, -0.7162], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0850, -0.1256, -0.6020], grad_fn=<SubBackward0>)\n","Episode: 259760, Total Reward: 77876.00000000259, Actor Loss: -0.6927385926246643, Critic Loss: 0.1284841150045395\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 2. 1.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.1417, -0.2149, -0.1376], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2572, -0.2522, -0.2657], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2546, -0.2497,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1130, -0.0348,  1.1376], grad_fn=<SubBackward0>)\n","Episode: 259761, Total Reward: 77877.00000000259, Actor Loss: 0.5360023379325867, Critic Loss: 0.43606674671173096\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[2. 0. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[2. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","board after 7 moves\n","[[2. 1. 1.]\n"," [1. 0. 2.]\n"," [2. 0. 1.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [1. 2. 2.]\n"," [2. 0. 1.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.6954, 0.7113, 0.6523, 0.5559, 0.5766], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5190, 0.4996, 0.3961, 0.5168, 0.5514], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5138, 0.4946, 0.3922, 0.5117, 1.1918], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1816, -0.2166, -0.2601, -0.0442,  0.6152], grad_fn=<SubBackward0>)\n","Episode: 259762, Total Reward: 77877.1000000026, Actor Loss: 0.13870899379253387, Critic Loss: 0.10559424012899399\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [2. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 1. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 1. 0.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [2. 1. 0.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[1. 0. 2.]\n"," [2. 1. 0.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.1633, 0.1317, 0.0390, 0.0816], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0480, -0.1552, -0.0829, -0.0829], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0476, -0.1537, -0.0820,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2108, -0.2854, -0.1210,  0.9184], grad_fn=<SubBackward0>)\n","Episode: 259763, Total Reward: 77878.1000000026, Actor Loss: 0.14531582593917847, Critic Loss: 0.24600253999233246\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[0. 1. 2.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [0. 0. 2.]\n"," [1. 0. 1.]]\n","board after 6 moves\n","[[0. 1. 2.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","board after 7 moves\n","[[0. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","board after 8 moves\n","[[0. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","board after 9 moves\n","[[1. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5670, 0.5806, 0.6175, 0.6370, 0.5817], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4543, 0.4935, 0.4692, 0.5562, 0.4426], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4498, 0.4886, 0.4645, 0.5506, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1173, -0.0920, -0.1530, -0.0864,  0.4183], grad_fn=<SubBackward0>)\n","Episode: 259764, Total Reward: 77879.1000000026, Actor Loss: -0.021609803661704063, Critic Loss: 0.045620374381542206\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 2. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([ 0.0311, -0.0788, -0.0738], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2094, -0.2897, -0.2524], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2073, -0.2868,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2384, -0.2080,  1.0738], grad_fn=<SubBackward0>)\n","Episode: 259765, Total Reward: 77880.1000000026, Actor Loss: 0.5144495368003845, Critic Loss: 0.4177307188510895\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 1. 2.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 1. 2.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[1. 1. 2.]\n"," [1. 2. 0.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.2414, -0.3394, -0.3492, -0.2961], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3352, -0.4102, -0.3419, -0.3954], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3319, -0.4061, -0.3384,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0905, -0.0667,  0.0108,  1.2961], grad_fn=<SubBackward0>)\n","Episode: 259766, Total Reward: 77881.1000000026, Actor Loss: 0.42325228452682495, Critic Loss: 0.4231620728969574\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [2. 1. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [2. 1. 1.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [2. 1. 1.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[1. 0. 2.]\n"," [2. 1. 1.]\n"," [1. 0. 2.]]\n","board after 8 moves\n","[[1. 0. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[1. 1. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.5253, 0.4828, 0.5301, 0.4787, 0.5700], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4417, 0.4737, 0.5820, 0.5176, 0.4605], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4373, 0.4689, 0.5762, 0.5124, 1.0119], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0880, -0.0139,  0.0461,  0.0337,  0.4419], grad_fn=<SubBackward0>)\n","Episode: 259767, Total Reward: 77881.2000000026, Actor Loss: 0.1925635188817978, Critic Loss: 0.041292063891887665\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 2. 0.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 1. 1.]\n"," [2. 2. 0.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [0. 1. 1.]\n"," [2. 2. 0.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [0. 1. 1.]\n"," [2. 2. 0.]]\n","board after 8 moves\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 0.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6509, 0.6387, 0.6527, 0.6773, 0.6808], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6115, 0.4958, 0.5648, 0.5962, 0.5974], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6054, 0.4909, 0.5591, 0.5902, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0454, -0.1479, -0.0935, -0.0871,  0.3192], grad_fn=<SubBackward0>)\n","Episode: 259768, Total Reward: 77882.2000000026, Actor Loss: -0.037462346255779266, Critic Loss: 0.028437014669179916\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 0. 2.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 0. 2.]\n"," [1. 0. 1.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [2. 0. 2.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[1. 1. 0.]\n"," [2. 0. 2.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[1. 1. 0.]\n"," [2. 2. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([0.1139, 0.0981, 0.0821, 0.1511], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0364, -0.0151, -0.1017, -0.0355], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0360, -0.0149, -0.1007, -0.9649], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1499, -0.1130, -0.1829, -1.1159], grad_fn=<SubBackward0>)\n","Episode: 259769, Total Reward: 77881.2000000026, Actor Loss: -0.7597644925117493, Critic Loss: 0.3285071849822998\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[2. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[2. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[2. 1. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[2. 1. 1.]\n"," [0. 1. 2.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[2. 1. 1.]\n"," [0. 1. 2.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[2. 1. 1.]\n"," [1. 1. 2.]\n"," [0. 0. 2.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [1. 1. 2.]\n"," [0. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [1. 1. 2.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.7221, 0.5985, 0.6623, 0.6321, 0.6284], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4960, 0.5053, 0.5027, 0.6456, 0.4914], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4910, 0.5002, 0.4977, 0.6391, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2311, -0.0983, -0.1646,  0.0071,  0.3716], grad_fn=<SubBackward0>)\n","Episode: 259770, Total Reward: 77882.2000000026, Actor Loss: -0.06599505990743637, Critic Loss: 0.045663975179195404\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[1. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[1. 0. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[1. 0. 1.]\n"," [1. 2. 2.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [1. 2. 2.]\n"," [0. 1. 2.]]\n","board after 8 moves\n","[[1. 0. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 2.]]\n","board after 9 moves\n","[[1. 1. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5919, 0.5762, 0.6214, 0.6777, 0.6251], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5023, 0.5171, 0.4493, 0.4062, 0.5283], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4973, 0.5119, 0.4448, 0.4022, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0946, -0.0642, -0.1766, -0.2755,  0.3749], grad_fn=<SubBackward0>)\n","Episode: 259771, Total Reward: 77883.2000000026, Actor Loss: -0.11728401482105255, Critic Loss: 0.05214741826057434\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 1.]]\n","board after 4 moves\n","[[2. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 1.]]\n","board after 5 moves\n","[[2. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.0375, -0.0648, -0.0396], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4302, -0.2583, -0.2359], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4259, -0.2557,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.3884, -0.1909,  1.0396], grad_fn=<SubBackward0>)\n","Episode: 259772, Total Reward: 77884.2000000026, Actor Loss: 0.531637966632843, Critic Loss: 0.4227198362350464\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 1. 2.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 6 moves\n","[[1. 1. 2.]\n"," [2. 1. 0.]\n"," [0. 2. 0.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [2. 1. 0.]\n"," [0. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0559, 0.1158, 0.1089, 0.0842], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0571, -0.1112, -0.2018, -0.0508], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0566, -0.1101, -0.1998,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1125, -0.2259, -0.3087,  0.9158], grad_fn=<SubBackward0>)\n","Episode: 259773, Total Reward: 77885.2000000026, Actor Loss: 0.05050879716873169, Critic Loss: 0.2494371235370636\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 1.]]\n","board after 4 moves\n","[[2. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[2. 0. 0.]\n"," [0. 1. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[2. 2. 0.]\n"," [0. 1. 0.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[2. 2. 1.]\n"," [0. 1. 0.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0642, 0.0739, 0.0556, 0.0380], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1326, -0.1353, -0.1812, -0.1313], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1312, -0.1339, -0.1794,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1955, -0.2078, -0.2349,  0.9620], grad_fn=<SubBackward0>)\n","Episode: 259774, Total Reward: 77886.2000000026, Actor Loss: 0.15846624970436096, Critic Loss: 0.2655228078365326\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [0. 1. 2.]]\n","board after 6 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [2. 1. 2.]]\n","board after 7 moves\n","[[0. 1. 1.]\n"," [2. 1. 0.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0816, -0.0656, -0.0580, -0.1087], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0753, -0.2203, -0.2197, -0.1511], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0746, -0.2181, -0.2175,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0070, -0.1525, -0.1595,  1.1087], grad_fn=<SubBackward0>)\n","Episode: 259775, Total Reward: 77887.2000000026, Actor Loss: 0.6981347799301147, Critic Loss: 0.3195149004459381\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [0. 1. 2.]\n"," [0. 1. 2.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.1119, -0.0557, -0.0279], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1002, -0.1906, -0.1902], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0992, -0.1887, -0.8117], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0128, -0.1330, -0.7837], grad_fn=<SubBackward0>)\n","Episode: 259776, Total Reward: 77886.2000000026, Actor Loss: -0.6007652282714844, Critic Loss: 0.2106974720954895\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[2. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[2. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[2. 2. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[2. 2. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]]\n","board after 6 moves\n","[[2. 2. 1.]\n"," [1. 2. 0.]\n"," [0. 1. 0.]]\n","board after 7 moves\n","[[2. 2. 1.]\n"," [1. 2. 0.]\n"," [1. 1. 0.]]\n","board after 8 moves\n","[[2. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","board after 9 moves\n","[[2. 2. 1.]\n"," [1. 2. 2.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5637, 0.6629, 0.5932, 0.5828, 0.5739], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4987, 0.5042, 0.4819, 0.5066, 0.4757], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4937, 0.4991, 0.4771, 0.5015, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0700, -0.1638, -0.1161, -0.0813,  0.4261], grad_fn=<SubBackward0>)\n","Episode: 259777, Total Reward: 77887.2000000026, Actor Loss: -0.05416427180171013, Critic Loss: 0.046674054116010666\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [1. 2. 0.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [1. 2. 2.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [1. 2. 2.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [0. 0. 1.]\n"," [1. 2. 2.]]\n","board after 7 moves\n","[[2. 1. 1.]\n"," [0. 0. 1.]\n"," [1. 2. 2.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6246, 0.7002, 0.6253, 0.6678, 0.6486], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5518, 0.5269, 0.6691, 0.6306, 0.6577], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5463, 0.5216, 0.6624, 0.6243, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0784, -0.1786,  0.0371, -0.0435,  0.3514], grad_fn=<SubBackward0>)\n","Episode: 259778, Total Reward: 77888.2000000026, Actor Loss: 0.03735116496682167, Critic Loss: 0.03296523541212082\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 1. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [1. 1. 0.]\n"," [0. 2. 2.]]\n","board after 5 moves\n","[[0. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 2. 2.]]\n","board after 6 moves\n","[[0. 0. 0.]\n"," [1. 1. 2.]\n"," [1. 2. 2.]]\n","board after 7 moves\n","[[1. 0. 0.]\n"," [1. 1. 2.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0019, -0.0047, -0.0042, -0.0603], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0363, -0.0974, -0.0471, -0.0941], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0360, -0.0965, -0.0466,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0341, -0.0918, -0.0424,  1.0603], grad_fn=<SubBackward0>)\n","Episode: 259779, Total Reward: 77889.2000000026, Actor Loss: 0.4688364267349243, Critic Loss: 0.2838934063911438\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 1. 0.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [2. 0. 1.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [2. 0. 1.]\n"," [2. 1. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.0629, -0.0081, -0.0887], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2056, -0.2338, -0.2676], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2036, -0.2314, -0.7351], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1407, -0.2234, -0.6464], grad_fn=<SubBackward0>)\n","Episode: 259780, Total Reward: 77888.2000000026, Actor Loss: -0.855360209941864, Critic Loss: 0.16249977052211761\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[2. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[2. 1. 0.]\n"," [1. 0. 1.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[2. 1. 0.]\n"," [1. 0. 1.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[2. 1. 1.]\n"," [1. 0. 1.]\n"," [0. 2. 2.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [1. 0. 1.]\n"," [2. 2. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.1868, -0.0838, -0.0891, -0.1098], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2443, -0.2196, -0.1890, -0.2089], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2418, -0.2174, -0.1871, -0.7932], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0550, -0.1335, -0.0979, -0.6834], grad_fn=<SubBackward0>)\n","Episode: 259781, Total Reward: 77887.2000000026, Actor Loss: -0.5491452217102051, Critic Loss: 0.12436486035585403\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 1.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [2. 2. 1.]]\n","board after 5 moves\n","[[1. 0. 1.]\n"," [0. 0. 0.]\n"," [2. 2. 1.]]\n","board after 6 moves\n","[[1. 0. 1.]\n"," [2. 0. 0.]\n"," [2. 2. 1.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([ 0.0681,  0.0586,  0.0718, -0.0039], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0132, -0.1240, -0.1254, -0.1482], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0131, -0.1227, -0.1242,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0812, -0.1813, -0.1960,  1.0039], grad_fn=<SubBackward0>)\n","Episode: 259782, Total Reward: 77888.2000000026, Actor Loss: 0.3666888475418091, Critic Loss: 0.2714407742023468\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 2.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [1. 2. 2.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 1. 0.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.0517, -0.0428, -0.1064], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2718, -0.1939, -0.2276], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2691, -0.1920,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2174, -0.1492,  1.1064], grad_fn=<SubBackward0>)\n","Episode: 259783, Total Reward: 77889.2000000026, Actor Loss: 0.6021345257759094, Critic Loss: 0.4312216341495514\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 2.]\n"," [1. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [1. 1. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [1. 1. 0.]\n"," [0. 2. 0.]]\n","board after 6 moves\n","[[0. 1. 2.]\n"," [1. 1. 0.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[0. 1. 2.]\n"," [1. 1. 1.]\n"," [0. 2. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0407, 0.0209, 0.0144, 0.0282], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0013, -0.1090, -0.1787, -0.1416], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0013, -0.1079, -0.1769,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0420, -0.1289, -0.1913,  0.9718], grad_fn=<SubBackward0>)\n","Episode: 259784, Total Reward: 77890.2000000026, Actor Loss: 0.337785929441452, Critic Loss: 0.24985267221927643\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 0.]\n"," [1. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 0.]\n"," [1. 1. 0.]\n"," [2. 0. 0.]]\n","board after 5 moves\n","[[0. 2. 0.]\n"," [1. 1. 0.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[0. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 0.]]\n","board after 7 moves\n","[[0. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","board after 8 moves\n","[[2. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","board after 9 moves\n","[[2. 2. 1.]\n"," [1. 1. 2.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.7348, 0.6130, 0.5844, 0.6418, 0.6192], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6454, 0.5601, 0.4762, 0.6197, 0.5779], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6390, 0.5545, 0.4714, 0.6135, 1.2442], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0958, -0.0585, -0.1129, -0.0283,  0.6250], grad_fn=<SubBackward0>)\n","Episode: 259785, Total Reward: 77890.30000000261, Actor Loss: 0.16756539046764374, Critic Loss: 0.08336080610752106\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 2. 0.]]\n","board after 6 moves\n","[[1. 2. 2.]\n"," [1. 0. 1.]\n"," [0. 2. 0.]]\n","board after 7 moves\n","[[1. 2. 2.]\n"," [1. 0. 1.]\n"," [0. 2. 1.]]\n","board after 8 moves\n","[[1. 2. 2.]\n"," [1. 0. 1.]\n"," [2. 2. 1.]]\n","board after 9 moves\n","[[1. 2. 2.]\n"," [1. 1. 1.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5740, 0.6140, 0.8018, 0.5857, 0.5885], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6064, 0.6240, 0.5401, 0.5708, 0.5357], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6003, 0.6178, 0.5347, 0.5651, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0263,  0.0037, -0.2671, -0.0207,  0.4115], grad_fn=<SubBackward0>)\n","Episode: 259786, Total Reward: 77891.30000000261, Actor Loss: 0.21661415696144104, Critic Loss: 0.04836117848753929\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [0. 2. 1.]\n"," [2. 1. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.2652, -0.2357, -0.1884], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2781, -0.3408, -0.4608], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2753, -0.3374, -0.5438], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0100, -0.1018, -0.3554], grad_fn=<SubBackward0>)\n","Episode: 259787, Total Reward: 77890.30000000261, Actor Loss: -0.4573664367198944, Critic Loss: 0.045595601201057434\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[1. 0. 1.]\n"," [1. 0. 2.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [1. 1. 2.]\n"," [0. 2. 2.]]\n","board after 8 moves\n","[[1. 0. 1.]\n"," [1. 1. 2.]\n"," [2. 2. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.0630, -0.0158, -0.0346, -0.0614], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1014, -0.1373, -0.0326, -0.0521], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1004, -0.1360, -0.0323, -0.9484], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0373, -0.1201,  0.0023, -0.8870], grad_fn=<SubBackward0>)\n","Episode: 259788, Total Reward: 77889.30000000261, Actor Loss: -0.6629139184951782, Critic Loss: 0.20063403248786926\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 2.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 0. 2.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [0. 2. 2.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [1. 2. 2.]\n"," [1. 0. 0.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.0536, -0.0748, -0.0522], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1666, -0.0404, -0.0879], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1649, -0.0400,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1113,  0.0347,  1.0522], grad_fn=<SubBackward0>)\n","Episode: 259789, Total Reward: 77890.30000000261, Actor Loss: 0.6514548659324646, Critic Loss: 0.3735548257827759\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 1. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 1. 0.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [2. 1. 0.]]\n","board after 5 moves\n","[[0. 0. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[0. 0. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 2.]]\n","board after 7 moves\n","[[1. 0. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 2.]]\n","board after 8 moves\n","[[1. 2. 0.]\n"," [1. 1. 2.]\n"," [2. 1. 2.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [1. 1. 2.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.6109, 0.6289, 0.6441, 0.5873, 0.6556], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5406, 0.6174, 0.5916, 0.5556, 0.5836], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5352, 0.6112, 0.5857, 0.5500, 1.2556], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0758, -0.0177, -0.0584, -0.0373,  0.5999], grad_fn=<SubBackward0>)\n","Episode: 259790, Total Reward: 77890.40000000261, Actor Loss: 0.1821993887424469, Critic Loss: 0.07415328919887543\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[2. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[2. 1. 1.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","board after 6 moves\n","[[2. 1. 1.]\n"," [0. 0. 1.]\n"," [2. 2. 0.]]\n","board after 7 moves\n","[[2. 1. 1.]\n"," [1. 0. 1.]\n"," [2. 2. 0.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [1. 2. 1.]\n"," [2. 2. 0.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [1. 2. 1.]\n"," [2. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6031, 0.6919, 0.6250, 0.6058, 0.6594], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4590, 0.6041, 0.4815, 0.5258, 0.4794], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4544, 0.5981, 0.4767, 0.5206, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1487, -0.0938, -0.1483, -0.0852,  0.3406], grad_fn=<SubBackward0>)\n","Episode: 259791, Total Reward: 77891.40000000261, Actor Loss: -0.08874060213565826, Critic Loss: 0.035232726484537125\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[0. 1. 1.]\n"," [0. 2. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 1. 1.]\n"," [0. 2. 0.]\n"," [0. 2. 0.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.2824, -0.1821, -0.2405], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2595, -0.3364, -0.3009], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2569, -0.3331,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0255, -0.1509,  1.2405], grad_fn=<SubBackward0>)\n","Episode: 259792, Total Reward: 77892.40000000261, Actor Loss: 0.6797115802764893, Critic Loss: 0.5207955241203308\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 0. 1.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[0. 0. 1.]\n"," [1. 2. 1.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[0. 0. 1.]\n"," [1. 2. 1.]\n"," [1. 2. 2.]]\n","board after 8 moves\n","[[2. 0. 1.]\n"," [1. 2. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.1058, -0.0581, -0.1332, -0.0735], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3781, -0.2989, -0.3158, -0.3720], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3744, -0.2959, -0.3127, -0.6317], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2685, -0.2378, -0.1795, -0.5582], grad_fn=<SubBackward0>)\n","Episode: 259793, Total Reward: 77891.40000000261, Actor Loss: -0.7111657857894897, Critic Loss: 0.1181187629699707\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 1. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [1. 0. 0.]\n"," [1. 0. 2.]]\n","board after 6 moves\n","[[0. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.2289, -0.2643, -0.2533, -0.2578], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3801, -0.3718, -0.4016, -0.4238], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3763, -0.3680, -0.3976,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1475, -0.1037, -0.1443,  1.2578], grad_fn=<SubBackward0>)\n","Episode: 259794, Total Reward: 77892.40000000261, Actor Loss: 0.3755698800086975, Critic Loss: 0.40886190533638\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [1. 2. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [1. 2. 2.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [1. 2. 2.]\n"," [0. 2. 1.]]\n","board after 7 moves\n","[[1. 0. 0.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0363, 0.0274, 0.0571, 0.0457], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1609, -0.2032, -0.2251, -0.1579], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1593, -0.2011, -0.2229,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1957, -0.2285, -0.2799,  0.9543], grad_fn=<SubBackward0>)\n","Episode: 259795, Total Reward: 77893.40000000261, Actor Loss: 0.06551697850227356, Critic Loss: 0.2699108123779297\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 2. 1.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [2. 0. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[0. 0. 1.]\n"," [2. 2. 0.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[0. 0. 1.]\n"," [2. 2. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.1187, 0.0915, 0.1113, 0.0765], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0869, -0.0163, -0.0665, -0.0089], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0860, -0.0161, -0.0659,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2047, -0.1076, -0.1771,  0.9235], grad_fn=<SubBackward0>)\n","Episode: 259796, Total Reward: 77894.40000000261, Actor Loss: 0.4463135004043579, Critic Loss: 0.23443832993507385\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 2. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[1. 2. 2.]\n"," [1. 2. 0.]\n"," [0. 0. 1.]]\n","board after 7 moves\n","[[1. 2. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.3778, -0.2656, -0.4424, -0.4227], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4410, -0.4464, -0.3746, -0.4319], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4366, -0.4420, -0.3709,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0588, -0.1764,  0.0715,  1.4227], grad_fn=<SubBackward0>)\n","Episode: 259797, Total Reward: 77895.40000000261, Actor Loss: 0.5417575836181641, Critic Loss: 0.5159293413162231\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[2. 0. 0.]\n"," [1. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[2. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[2. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[2. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 2. 1.]]\n","board after 7 moves\n","[[2. 0. 2.]\n"," [1. 0. 1.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[2. 0. 2.]\n"," [1. 2. 1.]\n"," [1. 2. 1.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 2. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.5216, 0.5452, 0.5195, 0.6193, 0.5341], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4289, 0.5380, 0.4046, 0.5140, 0.4151], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4246, 0.5326, 0.4006, 0.5088, 0.9220], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0970, -0.0126, -0.1189, -0.1105,  0.3879], grad_fn=<SubBackward0>)\n","Episode: 259798, Total Reward: 77895.50000000262, Actor Loss: -0.013292795047163963, Critic Loss: 0.037272773683071136\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 0. 1.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [1. 0. 1.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[0. 1. 2.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [1. 2. 1.]\n"," [2. 0. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.2724, -0.1639, -0.1583, -0.2140], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3219, -0.2862, -0.2897, -0.2304], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3187, -0.2834, -0.2868, -0.7719], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0463, -0.1194, -0.1285, -0.5579], grad_fn=<SubBackward0>)\n","Episode: 259799, Total Reward: 77894.50000000262, Actor Loss: -0.43994683027267456, Critic Loss: 0.08603686094284058\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 0. 2.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 1. 2.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 1. 2.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.2021, -0.1727, -0.2600], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3242, -0.2749, -0.3463], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3209, -0.2722, -0.6571], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1188, -0.0995, -0.3972], grad_fn=<SubBackward0>)\n","Episode: 259800, Total Reward: 77893.50000000262, Actor Loss: -0.5423634648323059, Critic Loss: 0.060588132590055466\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [1. 0. 2.]]\n","board after 6 moves\n","[[0. 1. 1.]\n"," [2. 2. 0.]\n"," [1. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 1.]\n"," [2. 2. 0.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0875, -0.0607, -0.0873, -0.0818], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3010, -0.2654, -0.2613, -0.2616], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2980, -0.2628, -0.2587,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2105, -0.2020, -0.1714,  1.0818], grad_fn=<SubBackward0>)\n","Episode: 259801, Total Reward: 77894.50000000262, Actor Loss: 0.1616837978363037, Critic Loss: 0.3212166130542755\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 2. 2.]\n"," [0. 1. 1.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[1. 2. 2.]\n"," [0. 1. 1.]\n"," [0. 2. 0.]]\n","board after 7 moves\n","[[1. 2. 2.]\n"," [0. 1. 1.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[1. 2. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 0.]]\n","board after 9 moves\n","[[1. 2. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6970, 0.6121, 0.5807, 0.6128, 0.5310], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5145, 0.4960, 0.4987, 0.5608, 0.4865], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5094, 0.4910, 0.4938, 0.5551, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1876, -0.1211, -0.0869, -0.0577,  0.4690], grad_fn=<SubBackward0>)\n","Episode: 259802, Total Reward: 77895.50000000262, Actor Loss: -0.052269406616687775, Critic Loss: 0.056130778044462204\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[1. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[1. 1. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 1. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[1. 1. 0.]\n"," [2. 0. 1.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[1. 1. 0.]\n"," [2. 1. 1.]\n"," [0. 2. 2.]]\n","board after 8 moves\n","[[1. 1. 0.]\n"," [2. 1. 1.]\n"," [2. 2. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([ 0.0348,  0.0327, -0.0268, -0.0078], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2367, -0.2224, -0.2138, -0.2032], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2344, -0.2202, -0.2116, -0.7988], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2691, -0.2530, -0.1848, -0.7910], grad_fn=<SubBackward0>)\n","Episode: 259803, Total Reward: 77894.50000000262, Actor Loss: -0.8650073409080505, Critic Loss: 0.19908839464187622\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [2. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 1.]]\n","board after 6 moves\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [2. 0. 1.]]\n","board after 7 moves\n","[[1. 2. 0.]\n"," [2. 0. 1.]\n"," [2. 1. 1.]]\n","board after 8 moves\n","[[1. 2. 0.]\n"," [2. 2. 1.]\n"," [2. 1. 1.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [2. 2. 1.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.7627, 0.6393, 0.6857, 0.6054, 0.6563], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5469, 0.5177, 0.6109, 0.5432, 0.4769], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5414, 0.5125, 0.6048, 0.5378, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2213, -0.1268, -0.0810, -0.0676,  0.3437], grad_fn=<SubBackward0>)\n","Episode: 259804, Total Reward: 77895.50000000262, Actor Loss: -0.05274287611246109, Critic Loss: 0.03885804861783981\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 1. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 2. 2.]\n"," [0. 1. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [0. 2. 2.]\n"," [0. 1. 1.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [2. 2. 2.]\n"," [0. 1. 1.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.1651, -0.1735, -0.1319], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2109, -0.1703, -0.1376], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2088, -0.1686, -0.8638], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0436,  0.0049, -0.7319], grad_fn=<SubBackward0>)\n","Episode: 259805, Total Reward: 77894.50000000262, Actor Loss: -0.41107872128486633, Critic Loss: 0.17919562757015228\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[1. 1. 0.]\n"," [1. 2. 0.]\n"," [2. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 0.]\n"," [1. 2. 0.]\n"," [2. 1. 2.]]\n","board after 8 moves\n","[[1. 1. 0.]\n"," [1. 2. 2.]\n"," [2. 1. 2.]]\n","board after 9 moves\n","[[1. 1. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5375, 0.4849, 0.5824, 0.5439, 0.5229], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4378, 0.4113, 0.4633, 0.3747, 0.4160], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4334, 0.4072, 0.4587, 0.3710, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1041, -0.0777, -0.1237, -0.1729,  0.4771], grad_fn=<SubBackward0>)\n","Episode: 259806, Total Reward: 77895.50000000262, Actor Loss: 0.05499475076794624, Critic Loss: 0.057934362441301346\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [2. 0. 1.]\n"," [0. 1. 0.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [2. 0. 1.]\n"," [2. 1. 0.]]\n","board after 7 moves\n","[[0. 2. 1.]\n"," [2. 0. 1.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0692, -0.0743, -0.0870, -0.0341], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1228, -0.1017, -0.1002, -0.1724], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1216, -0.1007, -0.0992,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0524, -0.0264, -0.0123,  1.0341], grad_fn=<SubBackward0>)\n","Episode: 259807, Total Reward: 77896.50000000262, Actor Loss: 0.6236199736595154, Critic Loss: 0.2682510018348694\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 1.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[0. 0. 2.]\n"," [1. 0. 1.]\n"," [0. 2. 1.]]\n","board after 6 moves\n","[[0. 0. 2.]\n"," [1. 2. 1.]\n"," [0. 2. 1.]]\n","board after 7 moves\n","[[0. 0. 2.]\n"," [1. 2. 1.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[2. 0. 2.]\n"," [1. 2. 1.]\n"," [1. 2. 1.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 2. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.5446, 0.6557, 0.6332, 0.5779, 0.6276], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5274, 0.4921, 0.4553, 0.4362, 0.4373], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5221, 0.4872, 0.4508, 0.4319, 0.9659], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0225, -0.1685, -0.1824, -0.1460,  0.3383], grad_fn=<SubBackward0>)\n","Episode: 259808, Total Reward: 77896.60000000263, Actor Loss: -0.08406306803226471, Critic Loss: 0.039593763649463654\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [0. 0. 1.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [2. 0. 1.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[1. 0. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[1. 2. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 0.]]\n","board after 9 moves\n","[[1. 2. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6671, 0.6407, 0.5704, 0.6182, 0.5777], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6053, 0.5716, 0.6251, 0.5099, 0.6197], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5993, 0.5659, 0.6188, 0.5048, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0678, -0.0748,  0.0484, -0.1133,  0.4223], grad_fn=<SubBackward0>)\n","Episode: 259809, Total Reward: 77897.60000000263, Actor Loss: 0.08497565984725952, Critic Loss: 0.04075227305293083\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [2. 2. 0.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [1. 0. 1.]\n"," [2. 2. 0.]]\n","board after 6 moves\n","[[0. 0. 1.]\n"," [1. 2. 1.]\n"," [2. 2. 0.]]\n","board after 7 moves\n","[[0. 1. 1.]\n"," [1. 2. 1.]\n"," [2. 2. 0.]]\n","board after 8 moves\n","[[0. 1. 1.]\n"," [1. 2. 1.]\n"," [2. 2. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.0355, -0.0729, -0.0515, -0.0662], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0183, -0.0912, -0.0699, -0.0390], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0181, -0.0903, -0.0692, -0.9614], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0174, -0.0174, -0.0177, -0.8951], grad_fn=<SubBackward0>)\n","Episode: 259810, Total Reward: 77896.60000000263, Actor Loss: -0.4715457856655121, Critic Loss: 0.200552836060524\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [1. 0. 2.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 1. 0.]\n"," [1. 2. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[0. 1. 0.]\n"," [1. 2. 2.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[2. 1. 0.]\n"," [1. 2. 2.]\n"," [1. 0. 0.]]\n","board after 7 moves\n","[[2. 1. 0.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 1. 0.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5333, 0.6109, 0.5641, 0.5388, 0.5689], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.4647, 0.5286, 0.4882, 0.5838, 0.4796], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.4600, 0.5234, 0.4833, 0.5779, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0732, -0.0875, -0.0808,  0.0391,  0.4311], grad_fn=<SubBackward0>)\n","Episode: 259811, Total Reward: 77897.60000000263, Actor Loss: 0.10361449420452118, Critic Loss: 0.04138948395848274\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 0.]\n"," [1. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 0.]\n"," [1. 2. 1.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[0. 2. 0.]\n"," [1. 2. 1.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[0. 2. 0.]\n"," [1. 2. 1.]\n"," [1. 2. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.1141, -0.1242, -0.1350], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2168, -0.2351, -0.2385], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2147, -0.2327, -0.7639], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1005, -0.1086, -0.6289], grad_fn=<SubBackward0>)\n","Episode: 259812, Total Reward: 77896.60000000263, Actor Loss: -0.5663249492645264, Critic Loss: 0.13913901150226593\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 2. 2.]\n"," [1. 0. 1.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[1. 2. 2.]\n"," [1. 0. 1.]\n"," [2. 0. 0.]]\n","board after 7 moves\n","[[1. 2. 2.]\n"," [1. 0. 1.]\n"," [2. 0. 1.]]\n","board after 8 moves\n","[[1. 2. 2.]\n"," [1. 2. 1.]\n"," [2. 0. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.2733, -0.2233, -0.2816, -0.2230], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0830, -0.1473, -0.0963, -0.1288], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0822, -0.1458, -0.0953, -0.8725], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.1911,  0.0775,  0.1863, -0.6495], grad_fn=<SubBackward0>)\n","Episode: 259813, Total Reward: 77895.60000000263, Actor Loss: -0.1429155468940735, Critic Loss: 0.12475937604904175\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 1.]]\n","board after 4 moves\n","[[0. 2. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 1.]]\n","board after 5 moves\n","[[0. 2. 0.]\n"," [1. 0. 1.]\n"," [2. 0. 1.]]\n","board after 6 moves\n","[[0. 2. 2.]\n"," [1. 0. 1.]\n"," [2. 0. 1.]]\n","board after 7 moves\n","[[0. 2. 2.]\n"," [1. 0. 1.]\n"," [2. 1. 1.]]\n","board after 8 moves\n","[[2. 2. 2.]\n"," [1. 0. 1.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.0766, -0.1393, -0.0734, -0.1003], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1345, -0.1676, -0.1792, -0.1297], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1332, -0.1659, -0.1774, -0.8716], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0566, -0.0266, -0.1040, -0.7713], grad_fn=<SubBackward0>)\n","Episode: 259814, Total Reward: 77894.60000000263, Actor Loss: -0.7940481305122375, Critic Loss: 0.15242116153240204\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[2. 1. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[2. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[2. 1. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[2. 1. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.2449, -0.2415, -0.2002], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3049, -0.2421, -0.3130], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3018, -0.2397, -0.6902], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0569,  0.0018, -0.4900], grad_fn=<SubBackward0>)\n","Episode: 259815, Total Reward: 77893.60000000263, Actor Loss: -0.36434006690979004, Critic Loss: 0.0811108723282814\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 0. 2.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 1. 2.]\n"," [0. 0. 1.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.0570,  0.0194,  0.0590], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1825, -0.2021, -0.2267], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1807, -0.2000,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1237, -0.2194,  0.9410], grad_fn=<SubBackward0>)\n","Episode: 259816, Total Reward: 77894.60000000263, Actor Loss: 0.47854843735694885, Critic Loss: 0.31630176305770874\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 2.]\n"," [1. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [1. 1. 0.]\n"," [2. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 2.]\n"," [1. 1. 0.]\n"," [2. 0. 0.]]\n","board after 6 moves\n","[[1. 0. 2.]\n"," [1. 1. 0.]\n"," [2. 0. 2.]]\n","board after 7 moves\n","[[1. 0. 2.]\n"," [1. 1. 1.]\n"," [2. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0576, -0.0905, -0.0579, -0.0827], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2270, -0.2969, -0.1921, -0.2640], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2247, -0.2939, -0.1902,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1671, -0.2034, -0.1323,  1.0827], grad_fn=<SubBackward0>)\n","Episode: 259817, Total Reward: 77895.60000000263, Actor Loss: 0.3923977017402649, Critic Loss: 0.3147485852241516\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 2.]]\n","board after 5 moves\n","[[0. 1. 1.]\n"," [0. 0. 1.]\n"," [0. 2. 2.]]\n","board after 6 moves\n","[[0. 1. 1.]\n"," [2. 0. 1.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[0. 1. 1.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.7005, 0.7080, 0.7138, 0.6537, 0.7830], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.7019, 0.5843, 0.6210, 0.5760, 0.5688], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6949, 0.5784, 0.6148, 0.5702, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0056, -0.1296, -0.0990, -0.0835,  0.2170], grad_fn=<SubBackward0>)\n","Episode: 259818, Total Reward: 77896.60000000263, Actor Loss: -0.027728021144866943, Critic Loss: 0.016141260042786598\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 2. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 2. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 2. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[1. 2. 1.]\n"," [0. 0. 2.]\n"," [0. 2. 1.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [0. 0. 2.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[1. 2. 1.]\n"," [2. 0. 2.]\n"," [1. 2. 1.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [2. 1. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.7792, 0.7404, 0.7164, 0.7597, 0.7524], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6566, 0.5791, 0.6654, 0.5543, 0.5666], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6501, 0.5733, 0.6587, 0.5487, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1291, -0.1671, -0.0577, -0.2110,  0.2476], grad_fn=<SubBackward0>)\n","Episode: 259819, Total Reward: 77897.60000000263, Actor Loss: -0.08681982755661011, Critic Loss: 0.03074730932712555\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 0. 1.]\n"," [2. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [1. 2. 1.]\n"," [2. 0. 0.]]\n","board after 5 moves\n","[[0. 0. 0.]\n"," [1. 2. 1.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[0. 2. 0.]\n"," [1. 2. 1.]\n"," [2. 1. 0.]]\n","board after 7 moves\n","[[0. 2. 0.]\n"," [1. 2. 1.]\n"," [2. 1. 1.]]\n","board after 8 moves\n","[[2. 2. 0.]\n"," [1. 2. 1.]\n"," [2. 1. 1.]]\n","board after 9 moves\n","[[2. 2. 1.]\n"," [1. 2. 1.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5408, 0.5642, 0.5028, 0.6240, 0.5891], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5167, 0.4675, 0.4519, 0.5057, 0.4400], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5115, 0.4628, 0.4473, 0.5006, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0293, -0.1014, -0.0555, -0.1234,  0.4109], grad_fn=<SubBackward0>)\n","Episode: 259820, Total Reward: 77898.60000000263, Actor Loss: 0.09679753333330154, Critic Loss: 0.039660580456256866\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[1. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[1. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 1. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 2.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.2415, -0.3024, -0.2139], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4385, -0.4683, -0.4749], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4342, -0.4636,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1926, -0.1612,  1.2139], grad_fn=<SubBackward0>)\n","Episode: 259821, Total Reward: 77899.60000000263, Actor Loss: 0.8612250685691833, Critic Loss: 0.5121719837188721\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 2. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 0.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 2. 0.]\n"," [1. 0. 1.]\n"," [0. 2. 0.]]\n","board after 6 moves\n","[[1. 2. 0.]\n"," [1. 2. 1.]\n"," [0. 2. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.0113, -0.0158,  0.0112], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0989, -0.1899, -0.1626], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0979, -0.1880, -0.8391], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0865, -0.1722, -0.8502], grad_fn=<SubBackward0>)\n","Episode: 259822, Total Reward: 77898.60000000263, Actor Loss: -0.7823291420936584, Critic Loss: 0.253351092338562\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 3 moves\n","[[0. 1. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 4 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [1. 0. 2.]]\n","board after 6 moves\n","[[0. 1. 1.]\n"," [2. 2. 0.]\n"," [1. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 1.]\n"," [2. 2. 0.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0581, -0.0551, -0.0147, -0.0239], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2033, -0.1443, -0.1205, -0.1743], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2013, -0.1428, -0.1193,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1432, -0.0878, -0.1046,  1.0239], grad_fn=<SubBackward0>)\n","Episode: 259823, Total Reward: 77899.60000000263, Actor Loss: 0.3034074306488037, Critic Loss: 0.2718978822231293\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[0. 1. 0.]\n"," [2. 0. 0.]\n"," [2. 1. 0.]]\n","board after 5 moves\n","[[0. 1. 1.]\n"," [2. 0. 0.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[2. 1. 1.]\n"," [2. 0. 0.]\n"," [2. 1. 0.]]\n","rewards:  [0, 0, -1]\n","values:  tensor([-0.1251, -0.1356, -0.0551], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1714, -0.1632, -0.2127], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1696, -0.1616, -0.7894], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0445, -0.0260, -0.7343], grad_fn=<SubBackward0>)\n","Episode: 259824, Total Reward: 77898.60000000263, Actor Loss: -0.642233669757843, Critic Loss: 0.18063350021839142\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 2.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 2.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [1. 2. 2.]]\n","board after 5 moves\n","[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [1. 2. 2.]]\n","board after 6 moves\n","[[0. 1. 0.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 7 moves\n","[[0. 1. 0.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","board after 8 moves\n","[[2. 1. 0.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.7904, 0.5941, 0.7074, 0.7419, 0.7045], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6704, 0.5721, 0.6412, 0.5850, 0.6252], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6637, 0.5664, 0.6348, 0.5792, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1267, -0.0276, -0.0726, -0.1627,  0.2955], grad_fn=<SubBackward0>)\n","Episode: 259825, Total Reward: 77899.60000000263, Actor Loss: -0.03410204499959946, Critic Loss: 0.02717389166355133\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 1. 2.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[1. 1. 2.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[1. 1. 2.]\n"," [2. 0. 1.]\n"," [0. 2. 0.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [2. 0. 1.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[1. 1. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.6235, 0.6430, 0.6551, 0.6379, 0.6224], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5732, 0.6651, 0.5206, 0.5918, 0.5507], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5675, 0.6585, 0.5153, 0.5859, 1.1903], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0560,  0.0155, -0.1398, -0.0521,  0.5679], grad_fn=<SubBackward0>)\n","Episode: 259826, Total Reward: 77899.70000000263, Actor Loss: 0.25876203179359436, Critic Loss: 0.06962497532367706\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [2. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [2. 1. 0.]\n"," [2. 0. 1.]]\n","board after 5 moves\n","[[0. 0. 1.]\n"," [2. 1. 0.]\n"," [2. 0. 1.]]\n","board after 6 moves\n","[[0. 0. 1.]\n"," [2. 1. 2.]\n"," [2. 0. 1.]]\n","board after 7 moves\n","[[0. 1. 1.]\n"," [2. 1. 2.]\n"," [2. 0. 1.]]\n","board after 8 moves\n","[[2. 1. 1.]\n"," [2. 1. 2.]\n"," [2. 0. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([ 0.0734,  0.1065, -0.0075,  0.0643], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([ 0.0190, -0.0214,  0.0142,  0.0064], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([ 0.0189, -0.0212,  0.0140, -1.0064], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0545, -0.1276,  0.0215, -1.0706], grad_fn=<SubBackward0>)\n","Episode: 259827, Total Reward: 77898.70000000263, Actor Loss: -0.6192400455474854, Critic Loss: 0.2914994955062866\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[1. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[1. 0. 1.]\n"," [2. 0. 0.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[1. 0. 1.]\n"," [2. 0. 0.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[1. 0. 1.]\n"," [2. 2. 0.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [2. 2. 1.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[1. 2. 1.]\n"," [2. 2. 1.]\n"," [1. 2. 0.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.0586, -0.1566, -0.1696, -0.1766], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0303, -0.0676, -0.0551, -0.0099], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0300, -0.0669, -0.0546, -0.9902], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0286,  0.0896,  0.1150, -0.8136], grad_fn=<SubBackward0>)\n","Episode: 259828, Total Reward: 77897.70000000263, Actor Loss: -0.37634050846099854, Critic Loss: 0.17100170254707336\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 2.]\n"," [1. 1. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 2.]\n"," [1. 1. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[0. 2. 2.]\n"," [1. 1. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[0. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 0.]]\n","board after 7 moves\n","[[0. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 0. 1.]]\n","board after 8 moves\n","[[0. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 2. 1.]]\n","board after 9 moves\n","[[1. 2. 2.]\n"," [1. 1. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6378, 0.5993, 0.7957, 0.6467, 0.6947], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5877, 0.5640, 0.6445, 0.5781, 0.5922], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5818, 0.5584, 0.6380, 0.5723, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0561, -0.0409, -0.1576, -0.0744,  0.3053], grad_fn=<SubBackward0>)\n","Episode: 259829, Total Reward: 77898.70000000263, Actor Loss: -0.016868680715560913, Critic Loss: 0.025687333196401596\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 2. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [0. 1. 2.]\n"," [0. 2. 1.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([0.1376, 0.1233, 0.1590], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2400, -0.2739, -0.2607], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2376, -0.2711,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.3752, -0.3944,  0.8410], grad_fn=<SubBackward0>)\n","Episode: 259830, Total Reward: 77899.70000000263, Actor Loss: -0.23755908012390137, Critic Loss: 0.33453163504600525\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[0. 0. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[2. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 0.]]\n","board after 7 moves\n","[[2. 1. 2.]\n"," [1. 2. 0.]\n"," [1. 0. 1.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 2. 2.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5072, 0.4914, 0.5242, 0.5267, 0.5426], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.3643, 0.4396, 0.4194, 0.3566, 0.4908], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.3606, 0.4352, 0.4152, 0.3531, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1465, -0.0562, -0.1090, -0.1736,  0.4574], grad_fn=<SubBackward0>)\n","Episode: 259831, Total Reward: 77900.70000000263, Actor Loss: -0.018766647204756737, Critic Loss: 0.05517350509762764\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 2. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [0. 2. 1.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [1. 2. 1.]\n"," [2. 0. 2.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [1. 2. 1.]\n"," [2. 0. 2.]]\n","board after 8 moves\n","[[1. 0. 1.]\n"," [1. 2. 1.]\n"," [2. 2. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.3182, -0.3063, -0.3171, -0.2339], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3387, -0.3632, -0.3115, -0.4187], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3353, -0.3596, -0.3084, -0.5855], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0171, -0.0533,  0.0087, -0.3516], grad_fn=<SubBackward0>)\n","Episode: 259832, Total Reward: 77899.70000000263, Actor Loss: -0.22709167003631592, Critic Loss: 0.031703460961580276\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 0. 2.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [0. 0. 2.]\n"," [2. 1. 0.]]\n","board after 5 moves\n","[[1. 1. 0.]\n"," [0. 0. 2.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[1. 1. 0.]\n"," [0. 0. 2.]\n"," [2. 1. 2.]]\n","board after 7 moves\n","[[1. 1. 0.]\n"," [0. 1. 2.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0598, 0.0850, 0.0413, 0.0431], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0501, -0.0889, -0.1096, -0.1261], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0496, -0.0880, -0.1085,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1094, -0.1731, -0.1498,  0.9569], grad_fn=<SubBackward0>)\n","Episode: 259833, Total Reward: 77900.70000000263, Actor Loss: 0.38991525769233704, Critic Loss: 0.2450006902217865\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[0. 1. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 2.]]\n","board after 5 moves\n","[[1. 1. 0.]\n"," [2. 0. 0.]\n"," [1. 0. 2.]]\n","board after 6 moves\n","[[1. 1. 0.]\n"," [2. 0. 0.]\n"," [1. 2. 2.]]\n","board after 7 moves\n","[[1. 1. 0.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 8 moves\n","[[1. 1. 0.]\n"," [2. 2. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[1. 1. 1.]\n"," [2. 2. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6786, 0.6068, 0.6675, 0.6835, 0.6487], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5652, 0.5793, 0.5995, 0.5925, 0.6272], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5596, 0.5735, 0.5935, 0.5865, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1190, -0.0333, -0.0739, -0.0969,  0.3513], grad_fn=<SubBackward0>)\n","Episode: 259834, Total Reward: 77901.70000000263, Actor Loss: -0.018724197521805763, Critic Loss: 0.030710140243172646\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 2. 0.]]\n","board after 3 moves\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","board after 4 moves\n","[[0. 2. 1.]\n"," [0. 0. 1.]\n"," [0. 2. 0.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [0. 0. 1.]\n"," [1. 2. 0.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [2. 0. 1.]\n"," [1. 2. 0.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [2. 0. 1.]\n"," [1. 2. 0.]]\n","board after 8 moves\n","[[1. 2. 1.]\n"," [2. 0. 1.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[1. 2. 1.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6944, 0.8089, 0.7950, 0.6821, 0.7370], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6865, 0.7355, 0.7593, 0.6608, 0.6371], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6796, 0.7281, 0.7517, 0.6542, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0148, -0.0807, -0.0433, -0.0279,  0.2630], grad_fn=<SubBackward0>)\n","Episode: 259835, Total Reward: 77902.70000000263, Actor Loss: 0.07997219264507294, Critic Loss: 0.015713009983301163\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 2. 0.]\n"," [0. 1. 0.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 2. 0.]\n"," [0. 1. 1.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [2. 2. 0.]\n"," [2. 1. 1.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [2. 2. 0.]\n"," [2. 1. 1.]]\n","board after 8 moves\n","[[1. 0. 1.]\n"," [2. 2. 2.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.0600, -0.0342, -0.0878, -0.1176], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2778, -0.2476, -0.3153, -0.2752], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2750, -0.2451, -0.3122, -0.7276], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.2150, -0.2109, -0.2244, -0.6100], grad_fn=<SubBackward0>)\n","Episode: 259836, Total Reward: 77901.70000000263, Actor Loss: -0.7225857973098755, Critic Loss: 0.1282845288515091\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 1. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [2. 1. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[0. 2. 0.]\n"," [2. 1. 0.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[0. 2. 1.]\n"," [2. 1. 0.]\n"," [0. 0. 1.]]\n","board after 6 moves\n","[[0. 2. 1.]\n"," [2. 1. 0.]\n"," [2. 0. 1.]]\n","board after 7 moves\n","[[1. 2. 1.]\n"," [2. 1. 0.]\n"," [2. 0. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.3019, 0.1796, 0.1207, 0.1645], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.0400,  0.0106, -0.0719, -0.0612], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.0396,  0.0105, -0.0712,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.3415, -0.1691, -0.1919,  0.8355], grad_fn=<SubBackward0>)\n","Episode: 259837, Total Reward: 77902.70000000263, Actor Loss: 0.07212883234024048, Critic Loss: 0.22003552317619324\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[2. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[2. 0. 1.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","board after 5 moves\n","[[2. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 0. 1.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [0. 2. 0.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[2. 0. 1.]\n"," [0. 2. 1.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.1701, -0.2171, -0.2137, -0.1971], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2801, -0.2625, -0.2737, -0.2716], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2773, -0.2599, -0.2709,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1071, -0.0428, -0.0573,  1.1971], grad_fn=<SubBackward0>)\n","Episode: 259838, Total Reward: 77903.70000000263, Actor Loss: 0.623366117477417, Critic Loss: 0.36239004135131836\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[1. 2. 0.]\n"," [0. 2. 0.]\n"," [1. 0. 1.]]\n","board after 6 moves\n","[[1. 2. 0.]\n"," [0. 2. 2.]\n"," [1. 0. 1.]]\n","board after 7 moves\n","[[1. 2. 0.]\n"," [1. 2. 2.]\n"," [1. 0. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.2268, -0.1335, -0.2324, -0.2485], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3690, -0.2778, -0.3052, -0.2885], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3653, -0.2750, -0.3022,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1385, -0.1415, -0.0698,  1.2485], grad_fn=<SubBackward0>)\n","Episode: 259839, Total Reward: 77904.70000000263, Actor Loss: 0.4337570071220398, Critic Loss: 0.4007396697998047\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 0.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [0. 0. 2.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[2. 1. 0.]\n"," [0. 0. 2.]\n"," [0. 1. 0.]]\n","board after 5 moves\n","[[2. 1. 0.]\n"," [0. 1. 2.]\n"," [0. 1. 0.]]\n","rewards:  [0, 0, 1]\n","values:  tensor([-0.0896, -0.1062, -0.0201], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2756, -0.2370, -0.2240], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2729, -0.2347,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1833, -0.1285,  1.0201], grad_fn=<SubBackward0>)\n","Episode: 259840, Total Reward: 77905.70000000263, Actor Loss: 0.6747901439666748, Critic Loss: 0.3635549247264862\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[2. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[2. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[2. 2. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[2. 2. 1.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[2. 2. 1.]\n"," [1. 0. 0.]\n"," [1. 0. 2.]]\n","board after 7 moves\n","[[2. 2. 1.]\n"," [1. 0. 1.]\n"," [1. 0. 2.]]\n","board after 8 moves\n","[[2. 2. 1.]\n"," [1. 2. 1.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.1186, -0.1883, -0.1707, -0.1864], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3000, -0.3472, -0.3908, -0.3582], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2970, -0.3437, -0.3869, -0.6453], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1784, -0.1555, -0.2162, -0.4589], grad_fn=<SubBackward0>)\n","Episode: 259841, Total Reward: 77904.70000000263, Actor Loss: -0.5563995838165283, Critic Loss: 0.07833005487918854\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 2. 1.]]\n","board after 5 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[2. 0. 0.]\n"," [1. 2. 0.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[2. 1. 0.]\n"," [1. 2. 0.]\n"," [1. 2. 1.]]\n","board after 8 moves\n","[[2. 1. 0.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","board after 9 moves\n","[[2. 1. 1.]\n"," [1. 2. 2.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.5537, 0.4756, 0.4858, 0.5484, 0.4942], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.3822, 0.4696, 0.3822, 0.5369, 0.3781], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.3784, 0.4649, 0.3784, 0.5315, 0.8487], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1753, -0.0106, -0.1075, -0.0168,  0.3545], grad_fn=<SubBackward0>)\n","Episode: 259842, Total Reward: 77904.80000000264, Actor Loss: 0.05824422091245651, Critic Loss: 0.03366382420063019\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [2. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [2. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [1. 2. 0.]\n"," [2. 0. 0.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [1. 2. 0.]\n"," [2. 1. 0.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [1. 2. 2.]\n"," [2. 1. 0.]]\n","board after 7 moves\n","[[1. 0. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 0.]]\n","board after 8 moves\n","[[1. 0. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 2.]]\n","board after 9 moves\n","[[1. 1. 1.]\n"," [1. 2. 2.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5166, 0.5753, 0.4820, 0.4759, 0.4928], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.3885, 0.4652, 0.4263, 0.4681, 0.3971], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.3847, 0.4605, 0.4220, 0.4634, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1320, -0.1148, -0.0600, -0.0125,  0.5072], grad_fn=<SubBackward0>)\n","Episode: 259843, Total Reward: 77905.80000000264, Actor Loss: 0.10867118835449219, Critic Loss: 0.05831335112452507\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 0.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 1. 2.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [1. 2. 1.]\n"," [0. 0. 0.]]\n","board after 6 moves\n","[[0. 1. 2.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [1. 2. 1.]\n"," [0. 0. 2.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [1. 2. 1.]\n"," [2. 0. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.2874, -0.2704, -0.3267, -0.3550], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.4754, -0.4084, -0.3597, -0.4174], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.4707, -0.4043, -0.3561, -0.5868], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1833, -0.1339, -0.0293, -0.2318], grad_fn=<SubBackward0>)\n","Episode: 259844, Total Reward: 77904.80000000264, Actor Loss: -0.3192702531814575, Critic Loss: 0.0265323668718338\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 1.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 0. 1.]]\n","board after 3 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [0. 1. 1.]]\n","board after 4 moves\n","[[0. 0. 0.]\n"," [2. 0. 0.]\n"," [2. 1. 1.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 0. 0.]\n"," [2. 1. 1.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [2. 0. 2.]\n"," [2. 1. 1.]]\n","board after 7 moves\n","[[1. 0. 0.]\n"," [2. 1. 2.]\n"," [2. 1. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([0.0072, 0.0705, 0.0071, 0.0528], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1213, -0.0797, -0.1354, -0.0967], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1201, -0.0789, -0.1340,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1272, -0.1494, -0.1411,  0.9472], grad_fn=<SubBackward0>)\n","Episode: 259845, Total Reward: 77905.80000000264, Actor Loss: 0.35662907361984253, Critic Loss: 0.23887528479099274\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 1. 2.]\n"," [1. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [1. 1. 0.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[2. 1. 2.]\n"," [1. 1. 0.]\n"," [0. 0. 2.]]\n","board after 7 moves\n","[[2. 1. 2.]\n"," [1. 1. 0.]\n"," [1. 0. 2.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [1. 1. 0.]\n"," [1. 2. 2.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [1. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.5685, 0.6209, 0.6533, 0.5743, 0.5558], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5189, 0.4611, 0.4818, 0.4228, 0.4879], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5137, 0.4565, 0.4770, 0.4186, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0548, -0.1644, -0.1763, -0.1557,  0.4442], grad_fn=<SubBackward0>)\n","Episode: 259846, Total Reward: 77906.80000000264, Actor Loss: -0.08253954350948334, Critic Loss: 0.05653689056634903\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[1. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[1. 2. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[1. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 2. 0.]]\n","board after 5 moves\n","[[1. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 2. 1.]]\n","board after 6 moves\n","[[1. 2. 2.]\n"," [0. 0. 0.]\n"," [1. 2. 1.]]\n","board after 7 moves\n","[[1. 2. 2.]\n"," [1. 0. 0.]\n"," [1. 2. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.0674, -0.0330, -0.0656, -0.0655], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.2423, -0.1626, -0.1390, -0.1711], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.2399, -0.1610, -0.1376,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1725, -0.1280, -0.0720,  1.0655], grad_fn=<SubBackward0>)\n","Episode: 259847, Total Reward: 77907.80000000264, Actor Loss: 0.3506394326686859, Critic Loss: 0.2966647446155548\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 2 moves\n","[[0. 2. 0.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 3 moves\n","[[0. 2. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[0. 2. 0.]\n"," [2. 1. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[0. 2. 0.]\n"," [2. 1. 0.]\n"," [1. 1. 0.]]\n","board after 6 moves\n","[[0. 2. 0.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","board after 7 moves\n","[[1. 2. 0.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","board after 8 moves\n","[[1. 2. 2.]\n"," [2. 1. 2.]\n"," [1. 1. 0.]]\n","board after 9 moves\n","[[1. 2. 2.]\n"," [2. 1. 2.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6266, 0.6686, 0.7017, 0.6023, 0.7049], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.6572, 0.5842, 0.7183, 0.6005, 0.6022], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.6506, 0.5784, 0.7111, 0.5945, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0241, -0.0903,  0.0093, -0.0079,  0.2951], grad_fn=<SubBackward0>)\n","Episode: 259848, Total Reward: 77908.80000000264, Actor Loss: 0.07005207985639572, Critic Loss: 0.019187983125448227\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[2. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[2. 0. 1.]\n"," [0. 0. 0.]\n"," [1. 0. 0.]]\n","board after 4 moves\n","[[2. 0. 1.]\n"," [2. 0. 0.]\n"," [1. 0. 0.]]\n","board after 5 moves\n","[[2. 0. 1.]\n"," [2. 0. 0.]\n"," [1. 0. 1.]]\n","board after 6 moves\n","[[2. 0. 1.]\n"," [2. 2. 0.]\n"," [1. 0. 1.]]\n","board after 7 moves\n","[[2. 0. 1.]\n"," [2. 2. 0.]\n"," [1. 1. 1.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([-0.1300, -0.1134, -0.1232, -0.1087], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1079, -0.1485, -0.0772, -0.0995], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1068, -0.1470, -0.0765,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0231, -0.0336,  0.0467,  1.1087], grad_fn=<SubBackward0>)\n","Episode: 259849, Total Reward: 77909.80000000264, Actor Loss: 0.7920051217079163, Critic Loss: 0.3082527220249176\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 0.]]\n","board after 4 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 2.]]\n","board after 5 moves\n","[[0. 1. 2.]\n"," [0. 0. 1.]\n"," [0. 1. 2.]]\n","board after 6 moves\n","[[0. 1. 2.]\n"," [2. 0. 1.]\n"," [0. 1. 2.]]\n","board after 7 moves\n","[[0. 1. 2.]\n"," [2. 0. 1.]\n"," [1. 1. 2.]]\n","board after 8 moves\n","[[2. 1. 2.]\n"," [2. 0. 1.]\n"," [1. 1. 2.]]\n","board after 9 moves\n","[[2. 1. 2.]\n"," [2. 1. 1.]\n"," [1. 1. 2.]]\n","rewards:  [0, 0, 0, 0, 1]\n","values:  tensor([0.6389, 0.6649, 0.5894, 0.7337, 0.6196], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5839, 0.5888, 0.4933, 0.5598, 0.5564], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5781, 0.5829, 0.4883, 0.5542, 1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.0608, -0.0820, -0.1010, -0.1795,  0.3804], grad_fn=<SubBackward0>)\n","Episode: 259850, Total Reward: 77910.80000000264, Actor Loss: 0.024701427668333054, Critic Loss: 0.03951122239232063\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 1.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[0. 1. 1.]\n"," [0. 0. 2.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[0. 1. 1.]\n"," [2. 0. 2.]\n"," [0. 0. 0.]]\n","board after 5 moves\n","[[0. 1. 1.]\n"," [2. 0. 2.]\n"," [1. 0. 0.]]\n","board after 6 moves\n","[[0. 1. 1.]\n"," [2. 0. 2.]\n"," [1. 0. 2.]]\n","board after 7 moves\n","[[1. 1. 1.]\n"," [2. 0. 2.]\n"," [1. 0. 2.]]\n","rewards:  [0, 0, 0, 1]\n","values:  tensor([ 0.0227, -0.0239,  0.0216, -0.0403], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.1227, -0.1358, -0.1184, -0.1418], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.1215, -0.1344, -0.1173,  1.0000], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1442, -0.1105, -0.1388,  1.0403], grad_fn=<SubBackward0>)\n","Episode: 259851, Total Reward: 77911.80000000264, Actor Loss: 0.335127592086792, Critic Loss: 0.28360986709594727\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 1. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 1. 2.]\n"," [0. 0. 0.]\n"," [0. 1. 2.]]\n","board after 6 moves\n","[[1. 1. 2.]\n"," [0. 0. 0.]\n"," [2. 1. 2.]]\n","board after 7 moves\n","[[1. 1. 2.]\n"," [0. 0. 1.]\n"," [2. 1. 2.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [0. 2. 1.]\n"," [2. 1. 2.]]\n","rewards:  [0, 0, 0, -1]\n","values:  tensor([-0.3495, -0.2258, -0.2395, -0.2069], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([-0.3285, -0.3426, -0.3139, -0.3917], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([-0.3252, -0.3392, -0.3108, -0.6122], grad_fn=<AddBackward0>)\n","advantage:  tensor([ 0.0242, -0.1134, -0.0713, -0.4054], grad_fn=<SubBackward0>)\n","Episode: 259852, Total Reward: 77910.80000000264, Actor Loss: -0.3065423369407654, Critic Loss: 0.04571230709552765\n","board after 0 moves\n","[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","board after 1 moves\n","[[0. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 0.]]\n","board after 2 moves\n","[[0. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","board after 3 moves\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 0.]]\n","board after 4 moves\n","[[1. 0. 0.]\n"," [2. 0. 1.]\n"," [0. 0. 2.]]\n","board after 5 moves\n","[[1. 0. 0.]\n"," [2. 1. 1.]\n"," [0. 0. 2.]]\n","board after 6 moves\n","[[1. 0. 0.]\n"," [2. 1. 1.]\n"," [0. 2. 2.]]\n","board after 7 moves\n","[[1. 1. 0.]\n"," [2. 1. 1.]\n"," [0. 2. 2.]]\n","board after 8 moves\n","[[1. 1. 2.]\n"," [2. 1. 1.]\n"," [0. 2. 2.]]\n","board after 9 moves\n","[[1. 1. 2.]\n"," [2. 1. 1.]\n"," [1. 2. 2.]]\n","rewards:  [0, 0, 0, 0, 0.1]\n","values:  tensor([0.6495, 0.6800, 0.6980, 0.5934, 0.6303], grad_fn=<SqueezeBackward1>)\n","next values:  tensor([0.5530, 0.6999, 0.5780, 0.5240, 0.6411], grad_fn=<SqueezeBackward1>)\n","td target:  tensor([0.5475, 0.6929, 0.5722, 0.5187, 1.3694], grad_fn=<AddBackward0>)\n","advantage:  tensor([-0.1020,  0.0129, -0.1258, -0.0747,  0.7391], grad_fn=<SubBackward0>)\n"]}],"source":["env = TicTacToe()\n","rewards = []\n","input_dim = 27  # State dimension\n","action_dim = 9  # Number of actions\n","episode_rewards = []\n","agent = A2CAgent(input_dim, action_dim)\n","train(env, agent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhvRg6iBlH_1"},"outputs":[],"source":["cumulative_episode_rewards = []\n","for i in range(len(episode_rewards)):\n","  if i == 0:\n","    cumulative_episode_rewards.append(episode_rewards[i])\n","  else:\n","    cumulative_episode_rewards.append(cumulative_episode_rewards[i-1] + episode_rewards[i])\n","\n","\n","plt.plot(cumulative_episode_rewards)\n","plt.title('Training Reward Over Episodes')\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wq_IDAbn1m49"},"outputs":[],"source":["# Example usage (assuming you have an environment)\n","# env = TicTacToe()\n","\n","\n","# criterion = nn.MSELoss()\n","# model = TransformerModel(state_dim = 9, action_dim = 9, hidden_dim=64, nhead=4, num_layers=2)\n","# optimizer = optim.Adam(agent.parameters(), lr=1e-6)\n","\n","# train(env, model, optimizer, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFtK-NhT6Epq"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","\n","class ReplayBufferDataset(Dataset):\n","    def __init__(self, data):  # data is a list of (s, a, r, s') tuples\n","        self.data = data\n","        self.state_scaler = StandardScaler() # For state normalization\n","\n","        # Fit the scaler on the states. Very important for offline RL.\n","        states = np.array([transition[0] for transition in data])\n","        self.state_scaler.fit(states)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        s, a, r, s_prime = self.data[idx]\n","\n","        # Normalize state. Very important for transformers\n","        s = self.state_scaler.transform(np.array(s).reshape(1, -1)).flatten() # Reshape for scaler\n","        s_prime = self.state_scaler.transform(np.array(s_prime).reshape(1, -1)).flatten() # Reshape for scaler\n","\n","        return torch.tensor(s, dtype=torch.float32), torch.tensor(a, dtype=torch.float32), torch.tensor(r, dtype=torch.float32), torch.tensor(s_prime, dtype=torch.float32)\n","\n","# Example usage (assuming 'experiences' is your list of tuples):\n","dataset = ReplayBufferDataset(experiences)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # shuffle for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AT4GI6526PQ8"},"outputs":[],"source":["# class TransformerModel(nn.Module):\n","#     def __init__(self, state_dim, action_dim, embedding_dim, nhead, num_layers, dropout):\n","#         super().__init__()\n","#         self.state_embedding = nn.Linear(state_dim, embedding_dim)\n","#         self.action_embedding = nn.Linear(action_dim, embedding_dim)\n","\n","#         self.transformer = nn.Transformer(\n","#             d_model=embedding_dim,\n","#             nhead=nhead,\n","#             num_encoder_layers=num_layers,\n","#             num_decoder_layers=num_layers,\n","#             dropout=dropout,\n","#         )\n","\n","#         self.output_layer = nn.Linear(embedding_dim, state_dim)  # Predict next state\n","\n","#     def forward(self, states, actions):\n","#         state_embeds = self.state_embedding(states)\n","#         action_embeds = self.action_embedding(actions)\n","\n","#         # Combine state and action embeddings (e.g., concatenation or addition)\n","#         #  Important: Add positional encodings if using a standard transformer\n","#         seq = torch.cat([state_embeds.unsqueeze(0), action_embeds.unsqueeze(0)], dim=0) # sequence of [S,A]\n","\n","#         # If you want to predict a sequence of the next state,\n","#         # you should provide a target sequence to the transformer's decoder\n","#         # (see the PyTorch Transformer documentation).\n","\n","#         output = self.transformer(seq)\n","\n","#         next_state_pred = self.output_layer(output[-1]) # Get last element for next state prediction\n","\n","#         return next_state_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwtd1UBObG47"},"outputs":[],"source":["# #Code that I don't currently understand\n","# def compute_gae(rewards, values, next_value, gamma=0.99, lam=0.95):\n","#     \"\"\"\n","#     Calculates Generalized Advantage Estimation (GAE).\n","#     \"\"\"\n","#     returns = torch.zeros_like(rewards)\n","#     advantages = torch.zeros_like(rewards)\n","\n","#     running_return = next_value\n","#     for t in reversed(range(len(rewards))):\n","#         running_return = rewards[t] + gamma * running_return\n","#         returns[t] = running_return\n","\n","#     running_advantage = 0\n","#     for t in reversed(range(len(rewards))):\n","#         delta = rewards[t] + gamma * values[t+1] - values[t]\n","#         running_advantage = delta + gamma * lam * running_advantage\n","#         advantages[t] = running_advantage\n","\n","#     advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n","#     # advantages = advantages\n","#     # returns = advantages + values\n","\n","#     return returns, advantages\n","\n","# def ppo_update(agent, optimizer, states, actions, log_probs, returns, advantages, clip_ratio=0.2):\n","#     \"\"\"\n","#     Performs a single PPO update step.\n","#     \"\"\"\n","#     logits, values = agent(states)\n","#     dist = Categorical(logits=logits)\n","#     new_log_probs = dist.log_prob(actions)\n","\n","#     ratio = torch.exp(new_log_probs - log_probs)\n","#     clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)\n","#     actor_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n","\n","#     value_loss = 0.5 * (returns - values).pow(2).mean()\n","\n","#     loss = actor_loss + 0.5 * value_loss\n","\n","#     optimizer.zero_grad()\n","#     loss.backward()\n","#     optimizer.step()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH-AQLf06p6F"},"outputs":[],"source":["# def train_transformer(model, dataloader, optimizer, criterion, num_epochs = 1000):\n","\n","#   for epoch in range(num_epochs):\n","#       for batch_states, batch_actions, batch_rewards, batch_next_states in dataloader:\n","#           optimizer.zero_grad()\n","\n","#           next_state_pred = model(batch_states, batch_actions)\n","\n","#           loss = criterion(next_state_pred, batch_next_states)\n","#           loss.backward()\n","#           optimizer.step()\n","\n","#       print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDQBkVAHGh4M"},"outputs":[],"source":["def compute_discounted_rewards(rewards, gamma=0.99):\n","    discounted_rewards = []\n","    discounted_reward = 0\n","    i = 0\n","    for reward in reversed(rewards):\n","        discounted_reward = reward + gamma * discounted_reward\n","        discounted_rewards.insert(0, discounted_reward)\n","        # print(i,\" discounted_rewards: \", discounted_rewards)\n","        i += 1\n","    discounted_rewards = torch.tensor(discounted_rewards)\n","\n","\n","    #discounted_rewards = discounted_rewards / (discounted_rewards.std() + 1e-5)\n","    #discounted_rewards = discounted_reward - discounted_rewards.mean() / (discounted_rewards.std() + 1e-5)\n","    # print(\"discounted rewards: \", discounted_rewards)\n","\n","    return discounted_rewards\n","\n","def train(env, agent, optimizer, replay_buffer_size=10000, batch_size=32, num_episodes=2000, gamma=0.99, lam=0.95, clip_ratio=0.2):\n","    replay_buffer = deque(maxlen=replay_buffer_size)\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        episode_reward = 0\n","        states_list, actions_list, rewards_list, log_probs_list, values_list = [], [], [], [], []\n","\n","        for t in range(500):  # Maximum episode length\n","            with torch.no_grad():\n","                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","                logits, value = agent(state_tensor)\n","                probs = torch.softmax(logits, dim=-1)\n","                m = Categorical(probs=probs)\n","                action = m.sample()\n","                log_prob = m.log_prob(action)\n","\n","            next_state, reward, done, _ = env.step(action.item())\n","            episode_reward += reward\n","\n","            states_list.append(state)\n","            actions_list.append(action.item())\n","            rewards_list.append(reward)\n","            log_probs_list.append(log_prob)\n","            values_list.append(value.item())\n","\n","            if done:\n","                break\n","            state = next_state\n","\n","        # Calculate returns and advantages\n","        next_value = 0.0 if done else agent(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0))[1].item()\n","        returns, advantages = compute_gae(rewards_list, values_list, next_value, gamma, lam)\n","\n","        # Store transition in replay buffer\n","        for i in range(len(states_list)):\n","            replay_buffer.append((states_list[i], actions_list[i], rewards_list[i], next_states[i], dones[i],\n","                                 log_probs_list[i], returns[i], advantages[i]))\n","\n","        # Sample a batch from the replay buffer\n","        batch = random.sample(replay_buffer, batch_size)\n","        states, actions, rewards, next_states, dones, log_probs, returns, advantages = zip(*batch)\n","        states = torch.tensor(states, dtype=torch.float32)\n","        actions = torch.tensor(actions, dtype=torch.long)\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        next_states = torch.tensor(next_states, dtype=torch.float32)\n","        dones = torch.tensor(dones, dtype=torch.bool)\n","        log_probs = torch.stack(log_probs)\n","        returns = torch.tensor(returns, dtype=torch.float32)\n","        advantages = torch.tensor(advantages, dtype=torch.float32)\n","\n","        # PPO update\n","        ppo_update(agent, optimizer, states, actions, log_probs, returns, advantages, clip_ratio)\n","\n","        print(f\"Episode: {episode}, Reward: {episode_reward}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNyeKPWmxKnbPKUsyFJc5QP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}