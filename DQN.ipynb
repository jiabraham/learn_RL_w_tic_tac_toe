{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOaRBUtzyuKw0tiit7c4Nv9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"rX9a9c-XT_yj","executionInfo":{"status":"ok","timestamp":1736203881372,"user_tz":300,"elapsed":12109,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"outputs":[],"source":["# importing all necessary libraries\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from google.colab import files\n","path = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/RL training data/nn_training_data_first_half.csv\"\n","training_data_first_half = pd.read_csv(path)\n","path = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/RL training data/nn_training_data_second_half.csv\"\n","training_data_second_half = pd.read_csv(path)\n","training_data = pd.concat([training_data_first_half, training_data_second_half])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olVtfm93UFDG","executionInfo":{"status":"ok","timestamp":1736204904191,"user_tz":300,"elapsed":8358,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}},"outputId":"0ca34d34-e09f-411c-da7a-d0e5a3119b55"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["print(len(training_data))\n","print(training_data.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hfj2k2fuUVT5","executionInfo":{"status":"ok","timestamp":1736204907296,"user_tz":300,"elapsed":133,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}},"outputId":"f8785b23-4acb-4c48-c830-c8ea1e3be391"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["6540829\n","   Unnamed: 0  0  1  2  3  4  5  6  7  8\n","0           0  0  0  0  0  0  0  0  0  0\n","1           1  0  0  0  1  0  0  0  0  0\n","2           2  2  0  0  1  0  0  0  0  0\n","3           3  2  0  0  1  0  0  1  0  0\n","4           4  2  0  0  1  0  0  1  2  0\n"]}]},{"cell_type":"code","source":["episode_rewards = []\n","class PolicyNetwork(nn.Module):\n","    def __init__(self):\n","        super(PolicyNetwork, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(4, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2),\n","            nn.Softmax(dim=-1),\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n"],"metadata":{"id":"CECYaEy4Ybm9","executionInfo":{"status":"ok","timestamp":1736204295388,"user_tz":300,"elapsed":216,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def compute_discounted_rewards(rewards, gamma=0.99):\n","    discounted_rewards = []\n","    R = 0\n","    for r in reversed(rewards):\n","        R = r + gamma * R\n","        discounted_rewards.insert(0, R)\n","    discounted_rewards = torch.tensor(discounted_rewards)\n","    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n","    return discounted_rewards\n","def train(env, policy, optimizer, episodes=1000):\n","  for episode in range(episodes):\n","      state = env.reset()\n","      log_probs = []\n","      rewards = []\n","      done = False\n","\n","      while not done:\n","          state = torch.FloatTensor(state).unsqueeze(0)\n","          probs = policy(state)\n","          m = Categorical(probs)\n","          action = m.sample()\n","          state, reward, done, _ = env.step(action.item())\n","\n","          log_probs.append(m.log_prob(action))\n","          rewards.append(reward)\n","          # Inside the train function, after an episode ends:\n","\n","          if done:\n","              episode_rewards.append(sum(rewards))\n","              discounted_rewards = compute_discounted_rewards(rewards)\n","              policy_loss = []\n","              for log_prob, Gt in zip(log_probs, discounted_rewards):\n","                  policy_loss.append(-log_prob * Gt)\n","              optimizer.zero_grad()\n","              policy_loss = torch.cat(policy_loss).sum()\n","              policy_loss.backward()\n","              optimizer.step()\n","\n","              if episode % 50 == 0:\n","                  print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n","              break"],"metadata":{"id":"3ZaDjbnV1luE","executionInfo":{"status":"ok","timestamp":1736204298779,"user_tz":300,"elapsed":188,"user":{"displayName":"Joseph Abraham","userId":"14852192777569956180"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["env = gym.make('CartPole-v1')\n","policy = PolicyNetwork()\n","optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n","\n","train(env, policy, optimizer)"],"metadata":{"id":"vY-AHUtO1pQQ"},"execution_count":null,"outputs":[]}]}
