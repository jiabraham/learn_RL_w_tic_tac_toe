{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBOhyI1rTCFK",
        "outputId": "1f9ff93f-3a5c-4a16-943e-69f75e887501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# importing all necessary libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import files\n",
        "path = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Deep Q Learning/nn_training_data_first_half.csv\"\n",
        "training_data_first_half = pd.read_csv(path)\n",
        "path = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Deep Q Learning/nn_training_data_second_half.csv\"\n",
        "training_data_second_half = pd.read_csv(path)\n",
        "training_data = np.delete(pd.concat([training_data_first_half, training_data_second_half]).to_numpy(), [0], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNk-urHhTIm6",
        "outputId": "4fac8549-e516-467a-eb18-5393c3adac04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(training_data))\n",
        "\n",
        "# training_data_list = []\n",
        "# for row in training_data:\n",
        "#   training_data_list.append(row.reshape(3,3))\n",
        "\n",
        "# for i in range(5):\n",
        "#   print(training_data_list[i])"
      ],
      "metadata": {
        "id": "zqzmXb0fTLHJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "  def __init__(self):\n",
        "    self.board = np.zeros((3, 3))\n",
        "\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((3, 3))\n",
        "    return self.board\n",
        "\n",
        "  def check_win(self, player):\n",
        "    # Check rows\n",
        "    for i in range(3):\n",
        "      if np.all(self.board[i, :] == player):\n",
        "        return player\n",
        "\n",
        "    # Check columns\n",
        "    for j in range(3):\n",
        "      if np.all(self.board[:, j] == player):\n",
        "        return player\n",
        "\n",
        "    #Check diagonal\n",
        "    if np.all(np.diag(self.board) == player):\n",
        "      return player\n",
        "    if np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "      return player\n",
        "\n",
        "    #Check tie\n",
        "    if np.all(self.board != 0):\n",
        "      return -1\n",
        "\n",
        "    return 0\n",
        "\n",
        "  # Check for empty places on board\n",
        "  def possibilities(self):\n",
        "    l = []\n",
        "\n",
        "    for i in range(len(self.board)):\n",
        "        for j in range(len(self.board)):\n",
        "\n",
        "            if self.board[i][j] == 0:\n",
        "                l.append((i, j))\n",
        "    return(l)\n",
        "\n",
        "  def random_place(self, player):\n",
        "    selection = self.possibilities()\n",
        "    current_loc = random.choice(selection)\n",
        "    self.board[current_loc] = player\n",
        "\n",
        "    return(self.board)\n",
        "\n",
        "  def step(self, action, player):\n",
        "    row, col = action\n",
        "    self.board[row, col] = player\n",
        "    done = self.check_win(player)\n",
        "    reward = 0\n",
        "    if done == 1 or done == 2:\n",
        "      reward = 1\n",
        "    return self.board, reward, done\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJTnNfR5TM5R",
        "outputId": "12808704-114a-4cd1-f846-f7fa11c5d4a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episode_rewards = []\n",
        "class PolicyNetworkTicTacToe(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetworkTicTacToe, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(9, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "dkSHDdmrTPTF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tic Tac Toe version\n",
        "def compute_discounted_rewards(rewards, gamma=0.99):\n",
        "    discounted_rewards = []\n",
        "    discounted_reward = 0\n",
        "    i = 0\n",
        "    for reward in reversed(rewards):\n",
        "        discounted_reward = reward + gamma * discounted_reward\n",
        "        discounted_rewards.insert(0, discounted_reward)\n",
        "        #print(i,\" discounted_rewards: \", discounted_rewards)\n",
        "        i += 1\n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    #print(\"reversed discounted_rewards: \", discounted_rewards)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    return discounted_rewards\n",
        "\n",
        "#%%debug\n",
        "debug = True\n",
        "\n",
        "def train(env, policy, optimizer, episodes=1000):\n",
        "  i = 0\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    if debug:\n",
        "      if i > 1:\n",
        "        break\n",
        "    state = env.reset()\n",
        "    if debug:\n",
        "      print(i,\" outer episode loop state: \", state)\n",
        "\n",
        "    log_probs = []\n",
        "    if debug:\n",
        "      print(i, \" outer episode loop log probabilities\", log_probs)\n",
        "\n",
        "\n",
        "    rewards = []\n",
        "    if debug:\n",
        "      print(i, \"outer episode loop rewards: \", rewards)\n",
        "    i += 1\n",
        "\n",
        "    done = 0\n",
        "    not_done_counter = 0\n",
        "    while done == 0:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" state: \", state)\n",
        "\n",
        "        probs = policy(state)\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" probs: \", probs)\n",
        "\n",
        "        m = Categorical(probs)\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" m: \", m)\n",
        "\n",
        "        action = m.sample()\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" action: \", action)\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \"step input action.item(): \", action.item())\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "        if debug:\n",
        "          print(\"inner episode loop after step \", not_done_counter, \"step output state: \", state)\n",
        "          print(\"inner episode loop after step \", not_done_counter, \"step output reward: \", reward)\n",
        "          print(\"inner episode loop after step \", not_done_counter, \"step output done: \", done)\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "        log_probs.append(m.log_prob(action))\n",
        "        rewards.append(reward)\n",
        "\n",
        "        not_done_counter += 1\n",
        "\n",
        "        # Inside the train function, after an episode ends:\n",
        "        if done:\n",
        "            print(\"DONE!!!\")\n",
        "            episode_rewards.append(sum(rewards))\n",
        "            discounted_rewards = compute_discounted_rewards(rewards)\n",
        "            policy_loss = []\n",
        "            for log_prob, Gt in zip(log_probs, disacounted_rewards):\n",
        "                policy_loss.append(-log_prob * Gt)\n",
        "            optimizer.zero_grad()\n",
        "            policy_loss = torch.cat(policy_loss).sum()\n",
        "            policy_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if episode % 50 == 0:\n",
        "                print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n",
        "                print(\"\")\n",
        "                print(\"\")\n",
        "                print(\"\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "bgyT9sEDTSCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJryd-8DTU1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}