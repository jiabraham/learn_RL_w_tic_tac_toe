{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqQOjb_La8-e"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import graphviz\n",
        "import seaborn as sns\n",
        "import time\n",
        "import math\n",
        "from torch.nn.utils import clip_grad_norm_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import files\n",
        "path_1 = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Deep Q Learning/nn_training_data_first_half.csv\"\n",
        "path_2 = \"/content/drive/MyDrive/Monarch/RL/Tic Tac Toe/Deep Q Learning/nn_training_data_second_half.csv\"\n",
        "\n",
        "offline_TTT_trajectories_first_half = pd.read_csv(path_1).to_numpy()\n",
        "offline_TTT_trajectories_second_half = pd.read_csv(path_2).to_numpy()\n",
        "\n",
        "offline_TTT_trajectories = np.concatenate((offline_TTT_trajectories_first_half, offline_TTT_trajectories_second_half), axis=0)\n",
        "print(offline_TTT_trajectories.shape)"
      ],
      "metadata": {
        "id": "NY8GMuCzbhuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define your environment\n",
        "class TicTacToe:\n",
        "  def __init__(self):\n",
        "    self.board = np.zeros((3, 3))\n",
        "\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((3, 3))\n",
        "    return self.board\n",
        "\n",
        "  def check_win(self, player):\n",
        "    # Check rows\n",
        "    for i in range(3):\n",
        "      if np.all(self.board[i, :] == player):\n",
        "        return player\n",
        "\n",
        "    # Check columns\n",
        "    for j in range(3):\n",
        "      if np.all(self.board[:, j] == player):\n",
        "        return player\n",
        "\n",
        "    #Check diagonal\n",
        "    if np.all(np.diag(self.board) == player):\n",
        "      return player\n",
        "    if np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "      return player\n",
        "\n",
        "    #Check tie\n",
        "    if np.all(self.board != 0):\n",
        "      return -1\n",
        "\n",
        "    return 0\n",
        "\n",
        "  # Check for empty places on board\n",
        "  def valid_state(self, potential_action):\n",
        "    valid_moves = []\n",
        "    for i in range(len(self.board)):\n",
        "        for j in range(len(self.board)):\n",
        "\n",
        "            if self.board[i][j] == 0:\n",
        "                valid_moves.append((i, j))\n",
        "    if potential_action in valid_moves:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "  # Check for empty places on board\n",
        "  def possibilities(self):\n",
        "    l = []\n",
        "    print(self.board)\n",
        "    for i in range(len(self.board)):\n",
        "        for j in range(len(self.board)):\n",
        "\n",
        "            if self.board[i][j] == 0:\n",
        "                l.append((i, j))\n",
        "    return(l)\n",
        "\n",
        "  def random_step(self, player):\n",
        "    selection = self.possibilities()\n",
        "    current_loc = random.choice(selection)\n",
        "    self.board[current_loc] = player\n",
        "    done = self.check_win(player)\n",
        "    reward = 0\n",
        "    if done == 2:\n",
        "      reward = -1\n",
        "    return self.board, reward, done\n",
        "\n",
        "  def network_step(self, action, player):\n",
        "    #print(action)\n",
        "    row, col = action\n",
        "    self.board[row, col] = player\n",
        "    done = self.check_win(player)\n",
        "    reward = 0\n",
        "    if done == 1:\n",
        "      reward = 1\n",
        "    return self.board, reward, done\n",
        "\n"
      ],
      "metadata": {
        "id": "3UUOCo5DbmME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerAgent(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, nhead, num_layers, dropout=0.1):\n",
        "        super(TransformerAgent, self).__init__()\n",
        "        self.transformer = nn.Transformer(d_model=hidden_dim, nhead=nhead,\n",
        "                                          num_decoder_layers=num_layers,\n",
        "                                          num_encoder_layers=num_layers,\n",
        "                                          dropout=dropout)\n",
        "        # Instead of an Embedding layer, use a Linear layer to handle continuous state values\n",
        "        self.state_embedding = nn.Linear(state_dim, hidden_dim)\n",
        "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed the input using the Linear layer\n",
        "        x = self.state_embedding(x.float())\n",
        "        # Reshape to (sequence_length, batch_size, embedding_dim)\n",
        "        x = x.view(1, 1, -1) # Reshape for Transformer\n",
        "\n",
        "        # Pass through the Transformer\n",
        "        output = self.transformer(x, x)[0, -1, :]\n",
        "        # Actor output (logits)\n",
        "        logits = self.actor(output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aMw1BwuLbrL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_to_tuple = {}\n",
        "for i in range(9):\n",
        "  tensor_to_tuple[i] = (i//3, i%3)\n",
        "print(tensor_to_tuple)"
      ],
      "metadata": {
        "id": "5mvzJuUibz9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import log\n",
        "#Tic Tac Toe version\n",
        "def compute_discounted_rewards(rewards, gamma=0.99):\n",
        "    discounted_rewards = []\n",
        "    discounted_reward = 0\n",
        "    i = 0\n",
        "    for reward in reversed(rewards):\n",
        "        discounted_reward = reward + gamma * discounted_reward\n",
        "        discounted_rewards.insert(0, discounted_reward)\n",
        "        #print(i,\" discounted_rewards: \", discounted_rewards)\n",
        "        i += 1\n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "\n",
        "    discounted_rewards = discounted_rewards / (discounted_rewards.std() + 1e-5)\n",
        "    #discounted_rewards = discounted_reward - discounted_rewards.mean() / (discounted_rewards.std() + 1e^-5)\n",
        "    # print(\"discounted rewards: \", discounted_rewards)\n",
        "    # print(\"discounted rewards mean: \", discounted_rewards.mean())\n",
        "    # print(\"discounted rewards std: \", discounted_rewards.std())\n",
        "\n",
        "    return discounted_rewards\n",
        "\n",
        "#%%debug\n",
        "debug = False\n",
        "\n",
        "def train(env, policy, optimizer, episodes=2000001):\n",
        "\n",
        "  i = 0\n",
        "  games_won = 0\n",
        "  games_lost = 0\n",
        "  games_skipped = 0\n",
        "  for episode in range(episodes):\n",
        "    print(\"trajectory: \", i)\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "\n",
        "    done = 0\n",
        "    not_done_counter = 0\n",
        "    while done == 0:\n",
        "        print(state)\n",
        "        state = state.flatten()\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" state: \", state)\n",
        "\n",
        "        probs = policy(state)\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" probs: \", probs)\n",
        "\n",
        "        action_mask = torch.zeros_like(probs)\n",
        "        for i in range(len(action_mask[0])): # Iterate through the actions in the mask\n",
        "          if env.valid_state(tensor_to_tuple[i]):\n",
        "            action_mask[0][i] = 1 # Apply mask to the specific action\n",
        "        action_probabilities = Categorical(probs * action_mask)\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" action_probabilities: \", action_probabilities)\n",
        "\n",
        "        action = action_probabilities.sample()\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \" action: \", action)\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "\n",
        "        if debug:\n",
        "          print(\"inner episode loop \", not_done_counter, \"step input action.item(): \", action.item())\n",
        "\n",
        "        state, reward, done = env.network_step(tensor_to_tuple[action.item()], player = 1)\n",
        "\n",
        "        #Only generate random move is state is not terminal\n",
        "        if done == 0:\n",
        "          state, reward, done = env.random_step(player = 2)\n",
        "        if debug:\n",
        "          print(\"inner episode loop after step \", not_done_counter)\n",
        "          print(\"step output state: \\n\", state)\n",
        "          print(\"step output reward: \", reward)\n",
        "          print(\"step output done: \", done)\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "        log_probs.append(action_probabilities.log_prob(action))\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Inside the train function, after an episode ends:\n",
        "        if done != 0:\n",
        "            i += 1\n",
        "            # print(state)\n",
        "            print(\"DONE!!! \", done)\n",
        "            if sum(rewards) == 1:\n",
        "              games_won += 1\n",
        "            elif sum(rewards) == -1:\n",
        "              games_lost += 1\n",
        "            episode_rewards.append(sum(rewards))\n",
        "            discounted_rewards = compute_discounted_rewards(rewards)\n",
        "\n",
        "            policy_loss = []\n",
        "            for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
        "                policy_loss.append(-log_prob * Gt)\n",
        "            optimizer.zero_grad()\n",
        "            policy_loss = torch.cat(policy_loss).sum()\n",
        "            policy_loss.backward()\n",
        "            optimizer.step()\n",
        "            episode_losses.append(policy_loss.item())\n",
        "\n",
        "\n",
        "            if episode % 50 == 0:\n",
        "                print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n",
        "                print(\"games_won: \", games_won)\n",
        "                print(\"games lost: \", games_lost)\n",
        "                print(\"games_skipped: \", games_skipped)\n",
        "                print(\"\")\n",
        "                print(\"\")\n",
        "                print(\"\")\n",
        "\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "omoN5cERb6be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Globals\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "\n",
        "#Driver code for training the model\n",
        "my_env = TicTacToe()\n",
        "print(\"\")\n",
        "policy = PolicyNetwork()\n",
        "print(\"Policy: \", policy)\n",
        "total_params = sum(p.numel() for p in policy.parameters())\n",
        "print(f\"Number of parameters: {total_params}\")\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-6)\n",
        "\n",
        "#train(env, policy, optimizer)\n",
        "train(my_env, policy, optimizer)"
      ],
      "metadata": {
        "id": "tCfXYRJwb9JW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}